#!/usr/bin/env python3
"""
ë°°ì¹˜ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸: runs/eval ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  .pth íŒŒì¼ë“¤ì„ ìë™ìœ¼ë¡œ í‰ê°€í•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    python scripts/batch_eval.py [--output_dir results] [--pattern "*epoch-1.pth"]
"""

import os
import glob
import subprocess
import argparse
import re
from pathlib import Path
import pandas as pd
from datetime import datetime

def parse_checkpoint_name(checkpoint_path):
    """
    ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ëª…ì—ì„œ ì„¤ì • ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    
    ì˜ˆì‹œ:
    - ns_window-5_sub-False__step-2000000.pth -> ns, window=5, subsample=False
    - hs_epoch-1.pth -> hs, epoch=1
    """
    filename = Path(checkpoint_path).stem
    
    # ê¸°ë³¸ê°’
    info = {
        'mode': 'ns',  # ns ë˜ëŠ” hs
        'window': 5,
        'subsample': True,
        'seed': 42,
        'epoch': None,
        'step': None
    }
    
    # ëª¨ë“œ ì¶”ì¶œ (ns ë˜ëŠ” hs)
    if filename.startswith('hs'):
        info['mode'] = 'hs'
    elif filename.startswith('ns'):
        info['mode'] = 'ns'
    
    # window í¬ê¸° ì¶”ì¶œ
    window_match = re.search(r'window-(\d+)', filename)
    if window_match:
        info['window'] = int(window_match.group(1))
    
    # subsample ì„¤ì • ì¶”ì¶œ
    if 'sub-False' in filename or 'subsample-off' in filename:
        info['subsample'] = False
    elif 'sub-True' in filename or 'subsample-on' in filename:
        info['subsample'] = True
    
    # seed ì¶”ì¶œ
    seed_match = re.search(r'seed-(\d+)', filename)
    if seed_match:
        info['seed'] = int(seed_match.group(1))
    
    # epoch ë˜ëŠ” step ì¶”ì¶œ
    epoch_match = re.search(r'epoch-(\d+)', filename)
    if epoch_match:
        info['epoch'] = int(epoch_match.group(1))
    
    step_match = re.search(r'step-(\d+)', filename)
    if step_match:
        info['step'] = int(step_match.group(1))
    
    return info

def find_matching_config(checkpoint_info, configs_dir="configs"):
    """
    ì²´í¬í¬ì¸íŠ¸ ì •ë³´ì— ë§ëŠ” config íŒŒì¼ì„ ì°¾ìŠµë‹ˆë‹¤.
    """
    mode = checkpoint_info['mode']
    window = checkpoint_info['window']
    subsample = 'on' if checkpoint_info['subsample'] else 'off'
    seed = checkpoint_info['seed']
    
    # ê°€ëŠ¥í•œ config íŒŒì¼ëª… íŒ¨í„´ë“¤
    patterns = [
        f"{mode}_window-{window}_subsample-{subsample}_seed-{seed}.yaml",
        f"{mode}_window-{window}_subsample-{subsample}_seed-42.yaml",  # fallback to seed 42
        f"{mode}_window-{window}_subsample-{subsample}.yaml",  # seed ì—†ëŠ” ë²„ì „
    ]
    
    for pattern in patterns:
        config_path = os.path.join(configs_dir, pattern)
        if os.path.exists(config_path):
            return config_path
    
    # ì°¾ì§€ ëª»í•œ ê²½ìš° ê°€ì¥ ìœ ì‚¬í•œ ê²ƒì„ ì°¾ê¸°
    config_files = glob.glob(f"{configs_dir}/{mode}_window-{window}_*.yaml")
    if config_files:
        return config_files[0]  # ì²« ë²ˆì§¸ ë§¤ì¹­ë˜ëŠ” íŒŒì¼ ë°˜í™˜
    
    return None

def run_evaluation(config_path, checkpoint_path, output_csv=None):
    """
    ë‹¨ì¼ ì²´í¬í¬ì¸íŠ¸ì— ëŒ€í•´ í‰ê°€ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.
    """
    # ê³ ì •ëœ ë°ì´í„°ì…‹ ê²½ë¡œë“¤
    wordsim_csv = "data/word_similarity/combined.csv"
    simlex_txt = "data/word_similarity/SimLex-999/SimLex-999.txt"
    analogy_txt = "data/word_similarity/word2vec/trunk/questions-words.txt"
    
    # eval.py ì‹¤í–‰ ëª…ë ¹ êµ¬ì„±
    cmd = [
        "python", "src/eval.py",
        config_path,
        checkpoint_path,
        wordsim_csv,
        simlex_txt,
        analogy_txt
    ]
    
    if output_csv:
        cmd.extend(["--save_csv", output_csv])
    
    print(f"ğŸš€ ì‹¤í–‰ ì¤‘: {' '.join(cmd)}")
    
    try:
        result = subprocess.run(cmd, capture_output=True, text=True, check=True)
        print(f"âœ… ì„±ê³µ: {checkpoint_path}")
        return True, result.stdout
    except subprocess.CalledProcessError as e:
        print(f"âŒ ì‹¤íŒ¨: {checkpoint_path}")
        print(f"ì—ëŸ¬: {e.stderr}")
        return False, e.stderr

def main():
    parser = argparse.ArgumentParser(description="ë°°ì¹˜ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸")
    parser.add_argument("--eval_dir", default="runs/eval", help="í‰ê°€í•  .pth íŒŒì¼ë“¤ì´ ìˆëŠ” ë””ë ‰í† ë¦¬")
    parser.add_argument("--configs_dir", default="configs", help="config íŒŒì¼ë“¤ì´ ìˆëŠ” ë””ë ‰í† ë¦¬")
    parser.add_argument("--output_dir", default="results/batch_eval", help="ê²°ê³¼ë¥¼ ì €ì¥í•  ë””ë ‰í† ë¦¬")
    parser.add_argument("--pattern", default="*epoch-1.pth", help="í‰ê°€í•  íŒŒì¼ íŒ¨í„´ (ì˜ˆ: *epoch-1.pth)")
    parser.add_argument("--dry_run", action="store_true", help="ì‹¤ì œ ì‹¤í–‰í•˜ì§€ ì•Šê³  ê³„íšë§Œ ì¶œë ¥")
    
    args = parser.parse_args()
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(args.output_dir, exist_ok=True)
    
    # .pth íŒŒì¼ë“¤ ì°¾ê¸°
    pattern_path = os.path.join(args.eval_dir, "**", args.pattern)
    checkpoint_files = glob.glob(pattern_path, recursive=True)
    
    print(f"ğŸ“ {len(checkpoint_files)}ê°œì˜ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤:")
    
    evaluation_plan = []
    
    for checkpoint_path in sorted(checkpoint_files):
        print(f"\nğŸ“„ ë¶„ì„ ì¤‘: {checkpoint_path}")
        
        # ì²´í¬í¬ì¸íŠ¸ ì •ë³´ ì¶”ì¶œ
        checkpoint_info = parse_checkpoint_name(checkpoint_path)
        print(f"   ì •ë³´: {checkpoint_info}")
        
        # ë§¤ì¹­ë˜ëŠ” config íŒŒì¼ ì°¾ê¸°
        config_path = find_matching_config(checkpoint_info, args.configs_dir)
        
        if config_path:
            print(f"   âœ… Config ì°¾ìŒ: {config_path}")
            
            # ì¶œë ¥ CSV íŒŒì¼ëª… ìƒì„±
            checkpoint_name = Path(checkpoint_path).stem
            output_csv = os.path.join(args.output_dir, f"{checkpoint_name}_results.csv")
            
            evaluation_plan.append({
                'checkpoint': checkpoint_path,
                'config': config_path,
                'output_csv': output_csv,
                'info': checkpoint_info
            })
        else:
            print(f"   âŒ ë§¤ì¹­ë˜ëŠ” config íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    
    print(f"\nğŸ“Š ì´ {len(evaluation_plan)}ê°œì˜ í‰ê°€ë¥¼ ì‹¤í–‰í•  ì˜ˆì •ì…ë‹ˆë‹¤.")
    
    if args.dry_run:
        print("\nğŸ” DRY RUN ëª¨ë“œ - ì‹¤ì œ ì‹¤í–‰í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤:")
        for i, plan in enumerate(evaluation_plan, 1):
            print(f"\n{i}. {plan['checkpoint']}")
            print(f"   Config: {plan['config']}")
            print(f"   Output: {plan['output_csv']}")
        return
    
    # ì‹¤ì œ í‰ê°€ ì‹¤í–‰
    results_summary = []
    successful = 0
    failed = 0
    
    for i, plan in enumerate(evaluation_plan, 1):
        print(f"\n{'='*60}")
        print(f"í‰ê°€ {i}/{len(evaluation_plan)}: {Path(plan['checkpoint']).name}")
        print(f"{'='*60}")
        
        success, output = run_evaluation(
            plan['config'], 
            plan['checkpoint'], 
            plan['output_csv']
        )
        
        if success:
            successful += 1
            results_summary.append({
                'checkpoint': plan['checkpoint'],
                'config': plan['config'],
                'status': 'SUCCESS',
                'output_csv': plan['output_csv']
            })
        else:
            failed += 1
            results_summary.append({
                'checkpoint': plan['checkpoint'],
                'config': plan['config'],
                'status': 'FAILED',
                'error': output
            })
    
    # ìµœì¢… ìš”ì•½
    print(f"\n{'='*60}")
    print(f"ë°°ì¹˜ í‰ê°€ ì™„ë£Œ!")
    print(f"{'='*60}")
    print(f"âœ… ì„±ê³µ: {successful}ê°œ")
    print(f"âŒ ì‹¤íŒ¨: {failed}ê°œ")
    print(f"ğŸ“ ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {args.output_dir}")
    
    # ìš”ì•½ íŒŒì¼ ì €ì¥
    summary_df = pd.DataFrame(results_summary)
    summary_path = os.path.join(args.output_dir, f"batch_eval_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv")
    summary_df.to_csv(summary_path, index=False)
    print(f"ğŸ“‹ ìš”ì•½ íŒŒì¼: {summary_path}")

if __name__ == "__main__":
    main()

