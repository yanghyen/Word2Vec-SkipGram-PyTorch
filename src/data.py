# src/data.py
from collections import Counter
import random
import csv
import torch
from torch.utils.data import Dataset, DataLoader, IterableDataset
import os
import pickle
from typing import Generator, List, Dict, Any

import numpy as np
from huffman_tree import HuffmanTree
import xml.etree.ElementTree as ET

def subsample_text(text, t=1e-3): 
    counter = Counter(text)
    total_count = len(text)
    freqs = {word: count / total_count for word, count in counter.items()}
    
    subsampled = []
    for word in text:
        f = freqs[word]
        p_drop = 1 - ((t / f) ** 0.5)
        p_drop = max(0, p_drop) 
        
        if random.random() > p_drop: 
            subsampled.append(word)
    return subsampled

TOKENIZED_TRAIN_PATH = "data/pretrain/tokenized_corpus.txt"
TOKEN_INDICES_PATH = "data/pretrain/token_indices_3.npy"

def word_stream_generator(file_path) -> Generator[List[str], None, None]:
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"Tokenized corpus file not found: {file_path}")
    
    with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
        for line in f:
            for token in line.strip().split():
                yield token
                
def build_vocab_stream(file_path, min_count=50):
    vocab_counter = Counter()
    print(f"Building vocab from stream: {file_path} (min_counter={min_count})")
    
    for token in word_stream_generator(file_path):
        vocab_counter[token] += 1
        
    vocab = {word: count for word, count in vocab_counter.items() if count >= min_count}
    word2idx = {word: i for i, word in enumerate(vocab.keys())}
    idx2word = {i: word for word, i in word2idx.items()}
    
    print(f"Built vocab: {len(vocab)} words (min_count={min_count})")
    word_freq = {word: vocab_counter[word] for word in vocab}
    
    return vocab, word2idx, idx2word, word_freq

def load_wordsim353(path):
    pairs = []
    with open(path, 'r', encoding='utf-8') as f:
        reader = csv.reader(f)
        next(reader)  
        for row in reader:
            w1, w2, sim = row[0].lower(), row[1].lower(), float(row[2])
            pairs.append((w1, w2, sim))
    return pairs

def load_simlex999(path):
    pairs = []
    with open(path, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f, delimiter='\t')
        for row in reader:
            w1, w2, sim = row['word1'].lower(), row['word2'].lower(), float(row['SimLex999'])
            pairs.append((w1, w2, sim))
    return pairs

def load_google_analogy(path):
    analogies = []
    with open(path, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if line.startswith(':') or len(line) == 0:
                continue
            words = line.lower().split()
            if len(words) == 4:
                analogies.append(tuple(words))
    return analogies

class SkipGramNSIterableDataset(IterableDataset):
    def __init__(self, file_path, word2idx, word_freq, device="cuda", neg_sample_size=5, window_size=2, subsample_t=1e-3):
        super().__init__()
        self.file_path = file_path
        self.word2idx = word2idx
        self.window_size = window_size
        self.neg_sample_size = neg_sample_size
        self.subsample_t = subsample_t
        self.vocab_size = len(word2idx)
        self.idx2word = {i: w for w, i in word2idx.items()} # í¸ì˜ë¥¼ ìœ„í•´ ì¶”ê°€
        
        # Negative Sampling í™•ë¥  ê³„ì‚°: min_countë¥¼ í†µê³¼í•œ ë‹¨ì–´ë“¤ì˜ ë¹ˆë„ ì‚¬ìš©
        freqs_list = [word_freq.get(self.idx2word.get(i), 0) for i in range(self.vocab_size)] 
        self.freqs_for_neg = torch.tensor(freqs_list, dtype=torch.float)
        self.freqs_for_neg[self.freqs_for_neg == 0] = 1e-30 # 0 ë°©ì§€
        self.sample_probs = self.freqs_for_neg.pow(0.75) / self.freqs_for_neg.pow(0.75).sum()
        # self.sample_probs = self.sample_probs.to(device)
        
        # Subsampling í™•ë¥  ê³„ì‚°
        self.total_count = sum(word_freq.values())
        self.freqs = {word: count / self.total_count for word, count in word_freq.items()}
        
        if not os.path.exists(self.file_path):
            raise FileNotFoundError(f"Binary index file not found: {self.file_path}. Run preprocessing first!")
        
        try:
            self.token_indices = np.load(self.file_path, mmap_mode='r')
            self.total_tokens = len(self.token_indices)
            print(f"Loaded token indices (mmap): Total tokens = {self.total_tokens:,}")
        except Exception as e:
            raise RuntimeError(f"Error loading token indices file via mmap: {e}")
        
    
    def __iter__(self):
        """í•™ìŠµ ìŒ (center, context, neg_samples)ì„ yield"""
        
        # ğŸŸ¢ [ìˆ˜ì •] ì›Œì»¤ë³„ ë°ì´í„° ë¶„í•  (mmap ë°°ì—´ ì¸ë±ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ë³‘ë ¬ ì²˜ë¦¬)
        worker_info = torch.utils.data.get_worker_info()
        overlap = self.window_size
        
        if worker_info is None:
            start_idx = 0
            end_idx = self.total_tokens
        else:
            per_worker = self.total_tokens // worker_info.num_workers
            
            start_idx = worker_info.id * per_worker
            end_idx = start_idx + per_worker
            
            if worker_info.id > 0:
                start_idx = max(0, start_idx - overlap)
            if worker_info.id < worker_info.num_workers - 1:
                end_idx = min(self.total_tokens, end_idx + overlap)     
            else:
                end_idx = self.total_tokens
        current_idx = start_idx + (overlap if worker_info and worker_info.id > 0 else 0)
        actual_end = end_idx - (overlap if worker_info and worker_info.id < worker_info.num_workers - 1 else 0)
        
        while current_idx < end_idx:
            # 1. ì¤‘ì‹¬ ë‹¨ì–´ ì„¤ì • ë° ì„œë¸Œìƒ˜í”Œë§
            center_idx = self.token_indices[current_idx] # mmap ë°°ì—´ì—ì„œ ì¸ë±ìŠ¤ ì ‘ê·¼
            center_token = self.idx2word[center_idx]
            
            # ì„œë¸Œìƒ˜í”Œë§ í™•ë¥ 
            f = self.freqs.get(center_token, 0)
            p_drop = 1 - ((self.subsample_t / f) ** 0.5) if f > 0 else 1
            if random.random() < p_drop: 
                current_idx += 1 # ë“œë¡­ëœ ê²½ìš°ì—ë„ ì¸ë±ìŠ¤ ì¦ê°€
                continue 

            # 2. ê°€ë³€ ìœˆë„ìš° ì„¤ì •
            actual_window = random.randint(1, self.window_size)
            
            # 3. ì£¼ë³€ ë‹¨ì–´(Context) ì„ íƒ
            # ë¬¸ë§¥ ì¸ë±ìŠ¤ ë²”ìœ„ ê³„ì‚° (í˜„ì¬ ì›Œì»¤ì˜ ë²”ìœ„(start_idx, end_idx)ë¥¼ ë²—ì–´ë‚˜ì§€ ì•Šë„ë¡ ì œí•œ)
            left_context_start = max(0, current_idx - actual_window)
            right_context_end = min(self.total_tokens, current_idx + actual_window + 1)
            
            for context_token_idx in range(left_context_start, right_context_end):
                if context_token_idx == current_idx:
                    continue
                    
                context_idx = self.token_indices[context_token_idx]

                # 4. ë„¤ê±°í‹°ë¸Œ ìƒ˜í”Œë§ (ë³€ê²½ ì—†ìŒ)
                neg_samples = torch.multinomial(
                    self.sample_probs,
                    self.neg_sample_size,
                    replacement=True
                )
                
                # 5. í•™ìŠµ ìŒ Yield
                yield torch.tensor(center_idx, dtype=torch.long), \
                      torch.tensor(context_idx, dtype=torch.long), \
                      neg_samples
                      
            current_idx += 1 # ì¤‘ì‹¬ ë‹¨ì–´ ì¸ë±ìŠ¤ ì¦ê°€ 
                          
                          
class SkipGramNSDataset(Dataset):
    def __init__(self, tokens, word2idx, vocab, neg_sample_size=5, window_size=2):
        
        self.tokens = tokens  
        self.word2idx = word2idx
        self.window_size = window_size
        self.neg_sample_size = neg_sample_size
        
        token_counter = Counter(self.tokens)
        
        idx_to_word_list = sorted(word2idx.items(), key=lambda item: item[1])
        
        freqs_list = []
        for word, idx in idx_to_word_list:
            freqs_list.append(token_counter.get(word, 0))
            
        freqs = torch.tensor(freqs_list, dtype=torch.float)
        freqs[freqs == 0] = 1e-30
        self.sample_probs = freqs.pow(0.75) / freqs.pow(0.75).sum()
        
    def __len__(self):
        return len(self.tokens)

    def __getitem__(self, idx):
        max_retries = 10
        for _ in range(max_retries):
            try:
                center_token = self.tokens[idx]
                center_idx = torch.tensor(self.word2idx[center_token], dtype=torch.long)

                window = random.randint(1, self.window_size)
                start = max(0, idx - window)
                end = min(len(self.tokens), idx + window + 1)
                
                context_candidates = list(range(start, end))
                if idx in context_candidates:
                    context_candidates.remove(idx)

                if not context_candidates:
                    j = random.choice([k for k in range(max(0, idx - 1), min(len(self.tokens), idx + 2)) if k != idx])
                else:
                    j = random.choice(context_candidates)

                context_token = self.tokens[j]
                context_idx = torch.tensor(self.word2idx[context_token], dtype=torch.long)

                neg_samples = torch.multinomial(
                    self.sample_probs,
                    self.neg_sample_size,
                    replacement=True
                )
                
                return center_idx, context_idx, neg_samples
                
            except KeyError:
                idx = random.randint(0, len(self.tokens) - 1)
                continue
                
        raise RuntimeError("Failed to sample a valid token after multiple retries.")

class SkipGramHSDataset(Dataset):
    def __init__(self, tokens, word2idx, window_size=2):
        self.tokens = tokens  
        self.word2idx = word2idx
        self.window_size = window_size

    def __len__(self):
        return len(self.tokens)

    def __getitem__(self, idx):
        center_token = self.tokens[idx]
        center_idx = torch.tensor(self.word2idx[center_token], dtype=torch.long)

        window = random.randint(1, self.window_size)
        start = max(0, idx - window)
        end = min(len(self.tokens), idx + window + 1)
        
        context_candidates = list(range(start, end))
        if idx in context_candidates:
          context_candidates.remove(idx)
        
        j = random.choice(context_candidates) if context_candidates else idx
        
        target_token = self.tokens[j]
        target_idx = torch.tensor(self.word2idx[target_token], dtype=torch.long)
        
        return center_idx, target_idx
    
class SkipGramHSIterableDataset(IterableDataset):
    """
    Hierarchical Softmax í•™ìŠµì„ ìœ„í•œ Skip-Gram Iterable Dataset.
    mmapëœ í† í° ì¸ë±ìŠ¤ íŒŒì¼ì„ ìŠ¤íŠ¸ë¦¬ë°í•˜ì—¬ (center_idx, path, code) ìŒì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    def __init__(self, file_path, word2idx, word_freq, path_table, code_table, window_size=2, subsample_t=1e-3):
        super().__init__()
        self.file_path = file_path
        self.word2idx = word2idx
        self.window_size = window_size
        self.subsample_t = subsample_t
        self.vocab_size = len(word2idx)
        self.idx2word = {i: w for w, i in word2idx.items()}
        
        # Huffman Tree ê²½ë¡œ/ì½”ë“œ í…Œì´ë¸”
        if path_table is None or code_table is None:
             raise ValueError("path_table and code_table must be provided for Hierarchical Softmax.")
        self.path_table = path_table # target indexì˜ ë¶€ëª¨ ë…¸ë“œ ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸ (ê²½ë¡œ)
        self.code_table = code_table # target indexì˜ ì´ì§„ ì½”ë“œ ë¦¬ìŠ¤íŠ¸
        
        # Subsampling í™•ë¥  ê³„ì‚° (SkipGramNSIterableDatasetê³¼ ë™ì¼)
        self.total_count = sum(word_freq.values())
        self.freqs = {word: count / self.total_count for word, count in word_freq.items()}
        
        if not os.path.exists(self.file_path):
            raise FileNotFoundError(f"Binary index file not found: {self.file_path}. Run preprocessing first!")
        
        try:
            # mmap_mode='r'ë¡œ ë©”ëª¨ë¦¬ ë§µí•‘í•˜ì—¬ ëŒ€ìš©ëŸ‰ íŒŒì¼ ì²˜ë¦¬ ë° ì›Œì»¤ ê°„ ê³µìœ 
            self.token_indices = np.load(self.file_path, mmap_mode='r')
            self.total_tokens = len(self.token_indices)
            print(f"Loaded token indices (mmap) for HS: Total tokens = {self.total_tokens:,}")
        except Exception as e:
            raise RuntimeError(f"Error loading token indices file via mmap: {e}")

    def __iter__(self):
        """í•™ìŠµ ìŒ (center_idx, target_path, target_code)ì„ yield"""
        
        # ğŸŸ¢ ì›Œì»¤ë³„ ë°ì´í„° ë¶„í•  (SkipGramNSIterableDatasetê³¼ ë™ì¼í•œ ë¡œì§)
        worker_info = torch.utils.data.get_worker_info()
        overlap = self.window_size # ê²½ê³„ì—ì„œ contextë¥¼ ë†“ì¹˜ì§€ ì•Šê¸° ìœ„í•¨

        if worker_info is None:
            start_idx = 0
            end_idx = self.total_tokens
        else:
            per_worker = self.total_tokens // worker_info.num_workers
            start_idx = worker_info.id * per_worker
            end_idx = start_idx + per_worker
            
            # ì›Œì»¤ ê²½ê³„ì—ì„œ ìœˆë„ìš° í¬ê¸°ë§Œí¼ ì˜¤ë²„ë©
            if worker_info.id > 0:
                start_idx = max(0, start_idx - overlap)
            if worker_info.id < worker_info.num_workers - 1:
                end_idx = min(self.total_tokens, end_idx + overlap) 
            else:
                end_idx = self.total_tokens # ë§ˆì§€ë§‰ ì›Œì»¤ëŠ” ëê¹Œì§€
                
        current_idx = start_idx + (overlap if worker_info and worker_info.id > 0 else 0) # ì‹¤ì œ ì‹œì‘ ì¸ë±ìŠ¤
        
        while current_idx < end_idx:
            # 1. ì¤‘ì‹¬ ë‹¨ì–´ ì„¤ì • ë° ì„œë¸Œìƒ˜í”Œë§ (SkipGramNSIterableDatasetê³¼ ë™ì¼)
            center_idx = self.token_indices[current_idx] 
            center_token = self.idx2word[center_idx]
            
            # ì„œë¸Œìƒ˜í”Œë§ í™•ë¥ 
            f = self.freqs.get(center_token, 0)
            p_drop = 1 - ((self.subsample_t / f) ** 0.5) if f > 0 else 1
            if random.random() < p_drop: 
                current_idx += 1 
                continue 

            # 2. ê°€ë³€ ìœˆë„ìš° ì„¤ì •
            actual_window = random.randint(1, self.window_size)
            
            # 3. ì£¼ë³€ ë‹¨ì–´(Target) ì„ íƒ
            # ë¬¸ë§¥ ì¸ë±ìŠ¤ ë²”ìœ„ ê³„ì‚° (í˜„ì¬ ì›Œì»¤ì˜ ë²”ìœ„(start_idx, end_idx)ë¥¼ ë²—ì–´ë‚˜ì§€ ì•Šë„ë¡ ì œí•œ)
            left_context_start = max(start_idx, current_idx - actual_window)
            right_context_end = min(end_idx, current_idx + actual_window + 1)
            
            for target_token_idx in range(left_context_start, right_context_end):
                if target_token_idx == current_idx:
                    continue
                
                target_idx = self.token_indices[target_token_idx]

                # 4. Hierarchical Softmax ê²½ë¡œ ë° ì½”ë“œ ê°€ì ¸ì˜¤ê¸°
                # target_idxëŠ” ë‹¨ì–´ ì¸ë±ìŠ¤ì´ë©°, path_table/code_tableì˜ ì¸ë±ìŠ¤ë¡œ ì‚¬ìš©ë¨
                # path_table[target_idx] -> ë¶€ëª¨ ë…¸ë“œ ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸ (ê²½ë¡œ)
                # code_table[target_idx] -> ì´ì§„ ì½”ë“œ ë¦¬ìŠ¤íŠ¸
                
                # word2vec êµ¬í˜„ì—ì„œ target_idxê°€ OOVì¼ ê²½ìš° ëŒ€ë¹„ ì½”ë“œê°€ í•„ìš”í•  ìˆ˜ ìˆìœ¼ë‚˜,
                # token_indicesëŠ” ì´ë¯¸ min_countë¥¼ í†µê³¼í•œ ë‹¨ì–´ë¡œ êµ¬ì„±ë˜ì—ˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.
                
                path = self.path_table[target_idx]
                code = self.code_table[target_idx]

                # 5. í•™ìŠµ ìŒ Yield
                # center_idx: ì¤‘ì‹¬ ë‹¨ì–´ ì¸ë±ìŠ¤
                # path: íƒ€ê²Ÿ ë‹¨ì–´ì˜ Huffman Tree ê²½ë¡œ (ë…¸ë“œ ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸)
                # code: ê²½ë¡œë¥¼ ë”°ë¼ê°€ë©° ì–»ëŠ” ì´ì§„ ì½”ë“œ (0 ë˜ëŠ” 1 ë¦¬ìŠ¤íŠ¸)
                yield torch.tensor(center_idx, dtype=torch.long), path, code
                
            current_idx += 1 # ì¤‘ì‹¬ ë‹¨ì–´ ì¸ë±ìŠ¤ ì¦ê°€
    
def collate_fn_hs(batch):
    centers, paths, codes = zip(*batch)
    
    max_len = max(len(p) for p in paths)
    batch_size = len(centers)
    
    padded_paths = torch.zeros(batch_size, max_len, dtype=torch.long)
    padded_codes = torch.zeros(batch_size, max_len, dtype=torch.float)
    masks = torch.zeros(batch_size, max_len, dtype=torch.float)
    
    for i, (p, c) in enumerate(zip(paths, codes)):
        l = len(p)
        padded_paths[i, :l] = torch.tensor(p)
        padded_codes[i, :l] = torch.tensor(c)
        masks[i, :l] = 1.0
    
    return torch.tensor(centers), padded_paths, padded_codes, masks

def get_dataloader(file_path, config, word2idx, word_freq, mode="ns", path_table=None, code_table=None):
    
    if mode == "ns":
        dataset = SkipGramNSIterableDataset(
            file_path=file_path, # ğŸ‘ˆ íŒŒì¼ ê²½ë¡œ ì „ë‹¬
            word2idx=word2idx, 
            word_freq=word_freq, # ğŸ‘ˆ ë¹ˆë„ìˆ˜ ì „ë‹¬,
            device="cuda",
            neg_sample_size=config.get("neg_sample_size", 5),
            window_size=config["window_size"]
        )
        collate_fn = None 
        
    elif mode == "hs":
        if path_table is None or code_table is None:
            raise ValueError("path_tableê³¼ code_tableì´ ì—†ì–´ìš”")
        dataset = SkipGramHSIterableDataset(
            file_path=file_path,
            word2idx=word2idx,
            word_freq=word_freq,
            path_table=path_table,
            code_table=code_table,
            window_size=config["window_size"]
        )
        collate_fn = collate_fn_hs
    else:
        raise ValueError("mode must be 'ns' or 'hs'")

    dataloader = DataLoader(
        dataset,
        batch_size=config["batch_size"],
        shuffle=False, 
        num_workers=config.get("num_workers", 16), 
        pin_memory=True,
        collate_fn=collate_fn
    )
    return dataloader