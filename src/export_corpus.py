from datasets import load_dataset
import os

# 1. Hugging Face ë°ì´í„°ì…‹ ìºì‹œì˜ ìµœìƒìœ„ ê²½ë¡œë¥¼ ì§€ì •í•©ë‹ˆë‹¤.
CACHE_DIR = "/home/ssai/Workspace/Word2Vec_repo/data/pretrain/huggingface_cache"

# 2. ìµœì¢… ì½”í¼ìŠ¤ íŒŒì¼ì´ ì €ì¥ë  ê²½ë¡œë¥¼ ì§€ì •í•©ë‹ˆë‹¤.
OUTPUT_PATH = "data/pretrain/word2vec_corpus_hf_half.txt"

# 3. ë°ì´í„°ì…‹ ë¡œë“œ (ìºì‹œëœ íŒŒì¼ì„ ìë™ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤)
print("ğŸ“˜ ìºì‹œëœ ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘...")
try:
    # dsëŠ” DatasetDict ê°ì²´ì…ë‹ˆë‹¤. (ì˜ˆ: {'train': Dataset(...)} )
    ds = load_dataset("lsb/enwiki20230101", cache_dir=CACHE_DIR)
except Exception as e:
    print(f"âŒ ë°ì´í„°ì…‹ ë¡œë“œ ì˜¤ë¥˜. CACHE_DIRì„ í™•ì¸í•´ì£¼ì„¸ìš”: {e}")
    ds = load_dataset("lsb/enwiki20230101")

ds_train = ds['train']
total_docs = len(ds_train)

half_docs = total_docs // 3  

ds_to_process = ds_train[:half_docs]['text'] 

print(f"ì´ {total_docs:,}ê°œì˜ ë¬¸ì„œ ì¤‘ {half_docs:,}ê°œë§Œ ë¡œë“œ ë° ì²˜ë¦¬ ì˜ˆì •.")
print(f"ë¬¸ì„œë¥¼ {OUTPUT_PATH} íŒŒì¼ë¡œ ìˆœì°¨ì ìœ¼ë¡œ ë‚´ë³´ë‚´ëŠ” ì¤‘...")

write_count = 0
with open(OUTPUT_PATH, 'w', encoding='utf-8') as f:
    for i, text in enumerate(ds_to_process): 
        stripped_text = text.strip()
        
        if stripped_text:
            f.write(stripped_text)
            f.write('\n\n') 
            write_count += 1
        
        if (i + 1) % 100000 == 0:
            print(f"    - {i+1:,}ë²ˆì§¸ ë¬¸ì„œê¹Œì§€ ì²˜ë¦¬ ì™„ë£Œ...")

print("\n---")
print(f"ì½”í¼ìŠ¤ íŒŒì¼ ìƒì„± ì™„ë£Œ: {OUTPUT_PATH}")
print(f"Total documents exported: {write_count:,}")
os.system(f"ls -lh {OUTPUT_PATH}") 