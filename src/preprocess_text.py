import os
import pickle
import re
from typing import Generator, List
from collections import Counter

import numpy as np
# NOTE: Assume build_vocab_stream, TOKEN_INDICES_PATH, TOKENIZED_TRAIN_PATH, word_stream_generator are imported from 'data'
from data import build_vocab_stream, TOKEN_INDICES_PATH, TOKENIZED_TRAIN_PATH, word_stream_generator 
import nltk
try:
    nltk.download("stopwords", quiet=True)
    nltk.download("punkt", quiet=True) 
    from nltk.tokenize import word_tokenize
    from nltk.corpus import stopwords
except ImportError:
    print("NLTK is not installed. Using simple split() for tokenization.")
    def word_tokenize(text):
        return re.findall(r"\b\w+\b", text) # ìµœì†Œí•œì˜ ë‹¨ì–´ ê²½ê³„ë¡œ ë¶„ë¦¬
    stopwords = set()

# -----------------------------
CORPUS_PATH = "data/pretrain/word2vec_corpus_hf_half.txt"

STOPWORDS = set(stopwords.words('english')) if 'stopwords' in locals() and stopwords else set()
# -----------------------------

def clean_token(token: str):
    """ì „ì²˜ë¦¬: URL, ìˆ«ì, íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬, ì†Œë¬¸ìí™”, ë¶ˆìš©ì–´ ì œê±°"""
    token = token.lower()

    # ì˜ë¬¸, ìˆ«ì, í•˜ì´í”ˆ, ì–´í¼ìŠ¤íŠ¸ë¡œí”¼ë§Œ í—ˆìš©
    token = re.sub(r"[^a-z0-9'-]", '', token)
    
    # ê¸¸ì´ê°€ ì§§ì€ í† í°ì´ë‚˜ ë¶ˆìš©ì–´ ì²˜ë¦¬
    if not token or token in STOPWORDS or token.strip() == '' or len(token) < 2:
        return None
    return token

def preprocess_tokens(tokens: list):
    """í† í° ë¦¬ìŠ¤íŠ¸ ì „ì²´ ì „ì²˜ë¦¬"""
    cleaned_tokens = []
    for t in tokens:
        ct = clean_token(t)
        if ct:
            cleaned_tokens.append(ct)
    return cleaned_tokens

def preprocess_text(text: str) -> list:
    """ë‹¨ì¼ ë¬¸ì„œ í…ìŠ¤íŠ¸ì— ëŒ€í•´ ì „ì²˜ë¦¬ ë° í† í°í™”ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
    # ì°¸ì¡° ì„¹ì…˜ ì œê±°
    text = re.sub(r'==\s*(References|External links|See also|Notes|Sources)\s*==.*', '', text, flags=re.DOTALL | re.IGNORECASE)
    # URL ì œê±°
    text = re.sub(r'https?://\S+|www\.\S+', '', text)
    tokens = word_tokenize(text)
    return preprocess_tokens(tokens)

def process_corpus_and_stream(path=CORPUS_PATH) -> Generator[List[str], None, None]:
    """
    ì›ë³¸ íŒŒì¼ì„ í•œ ì¤„ì”© ì½ì–´ ë¬¸ì„œë¥¼ ì¬êµ¬ì„±í•˜ê³ , ì „ì²˜ë¦¬ ë° í† í°í™”ëœ í† í° ë¦¬ìŠ¤íŠ¸ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ yieldí•©ë‹ˆë‹¤.
    (Vocab êµ¬ì¶•ìš© ì„ì‹œ íŒŒì¼ ìƒì„± ë° ì²« ë²ˆì§¸ ì¸ë±ì‹± ìŠ¤íŠ¸ë¦¼ì— ì‚¬ìš©)
    """
    if not os.path.exists(path):
        raise FileNotFoundError(f"Corpus file not found: {path}.")
    
    print(f"Starting streaming process from {path}.")
    
    doc_buffer = []
    doc_count = 0
    
    with open(path, "r", encoding="utf-8", errors="ignore") as f:
        for line in f:
            line = line.strip()
            
            if line:
                doc_buffer.append(line)
                
            if not line and doc_buffer:
                doc_text = " ".join(doc_buffer)
                tokens = preprocess_text(doc_text)
                
                if tokens:
                    yield tokens 
                    doc_count += 1
                doc_buffer = []
                
                if doc_count % 100000 == 0 and doc_count > 0:
                    print(f"Processed {doc_count:,} documents so far...")
                    
        # ë§ˆì§€ë§‰ ë¬¸ì„œ ì²˜ë¦¬
        if doc_buffer:
            doc_text = " ".join(doc_buffer)
            tokens = preprocess_text(doc_text)
            if tokens:
                yield tokens 
                doc_count += 1
    print(f"\nProcessing complete. Total documents processd: {doc_count:,}")


def word_stream_generator_from_tokenized_file(path: str) -> Generator[List[str], None, None]:
    """
    â­ ìµœì í™”: ì´ë¯¸ í† í°í™”ë˜ì–´ ì¤„ ë‹¨ìœ„ë¡œ ì €ì¥ëœ ì„ì‹œ íŒŒì¼ì—ì„œ í† í° ìŠ¤íŠ¸ë¦¼ì„ ìƒì„±í•©ë‹ˆë‹¤.
    ì´ í•¨ìˆ˜ëŠ” ìµœì¢… ì¸ë±ì‹± ì‹œ I/O ë° CPU ì—°ì‚°ì„ ì ˆì•½í•©ë‹ˆë‹¤.
    """
    print(f"Starting stream from pre-tokenized file: {path}")
    doc_count = 0
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if line:
                # ì´ë¯¸ í† í°í™”ë˜ì–´ ê³µë°±ìœ¼ë¡œ êµ¬ë¶„ëœ ë¬¸ìì—´ì´ë¯€ë¡œ, split()ë§Œ ì‚¬ìš©
                tokens = line.split() 
                yield tokens
                doc_count += 1
                
                if doc_count % 500000 == 0 and doc_count > 0:
                    print(f"Streamed {doc_count:,} tokenized documents so far...")


def save_token_indices_to_binary(
    token_stream: Generator[List[str], None, None],
    word2idx: dict,
    save_path=TOKEN_INDICES_PATH
):
    """
    Vocabì— ì—†ëŠ” ë‹¨ì–´ëŠ” ëª¨ë‘ <unk> ì¸ë±ìŠ¤ë¡œ ì¹˜í™˜í•˜ì—¬ ë°”ì´ë„ˆë¦¬ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    """
    print(f"Indexing corpus and saving to {save_path}...")

    all_indices = []
    total_tokens_count = 0
    
    # <unk> í† í°ì˜ ì¸ë±ìŠ¤ í™•ì¸ (ì¸ë±ìŠ¤ 0ìœ¼ë¡œ ê°€ì •)
    try:
        unk_idx = word2idx.get('<unk>', -1) 
        if unk_idx == -1:
             raise KeyError("<unk> token not found in word2idx. Check vocab building step.")
    except KeyError as e:
        print(f"âŒ ì˜¤ë¥˜: {e}. ì¸ë±ì‹±ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
        return

    for tokens in token_stream:
        indices = []
        for token in tokens:
            # í† í°ì´ word2idxì— ì—†ìœ¼ë©´ unk_idxë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
            idx = word2idx.get(token, unk_idx) 
            indices.append(idx)
            
        all_indices.extend(indices)
        
        total_tokens_count += len(indices)
        if total_tokens_count % 50000000 == 0 and total_tokens_count > 0:
            print(f"Tokens indexed so far: {total_tokens_count:,}")
    
    # NumPy ë°°ì—´ë¡œ ë³€í™˜ ë° ì €ì¥ (ë°”ì´ë„ˆë¦¬ íŒŒì¼)
    token_indices_array = np.array(all_indices, dtype=np.int32)
    
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    np.save(save_path, token_indices_array)
    
    print(f"\nâœ… Corpus indexing complete. Total indices: {len(token_indices_array):,}. Saved to {save_path}")

# -----------------------------

if __name__ == "__main__":
    
    try:
        # ----------------------------- 1. Vocab êµ¬ì¶• ë° ì„ì‹œ íŒŒì¼ ìƒì„± -----------------------------
        
        # A. ì›ë³¸ ì½”í¼ìŠ¤ë¥¼ ì½ì–´ í† í°í™”ëœ ë‚´ìš©ì„ ì„ì‹œ íŒŒì¼ì— ì €ì¥ (Vocab êµ¬ì¶•ìš©)
        # ì´ ë‹¨ê³„ì—ì„œë§Œ ë³µì¡í•œ ì „ì²˜ë¦¬ ê³¼ì •ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        print(f"Saving temporary tokenized corpus to {TOKENIZED_TRAIN_PATH} for vocab building...")
        total_temp_tokens = 0
        os.makedirs(os.path.dirname(TOKENIZED_TRAIN_PATH), exist_ok=True)
        with open(TOKENIZED_TRAIN_PATH, "w", encoding="utf-8") as f:
            temp_stream = process_corpus_and_stream(CORPUS_PATH)
            for tokens in temp_stream:
                f.write(" ".join(tokens) + "\n")
                total_temp_tokens += len(tokens)
        print(f"Temporary tokenized file created. Total tokens: {total_temp_tokens}")
        
        # B. ì„ì‹œ íŒŒì¼ë¡œ Vocab êµ¬ì¶• (min_count=5 ì´í•˜ ë‹¨ì–´ëŠ” ì œì™¸)
        VOCAB_MIN_COUNT = 50
        vocab, word2idx, idx2word, word_freq = build_vocab_stream(
            TOKENIZED_TRAIN_PATH,
            min_count=VOCAB_MIN_COUNT
        )
        
        # ----------------------------- 2. <unk> í† í° ê°•ì œ ì¶”ê°€ ë° ì¸ë±ìŠ¤ ì¬ì¡°ì • -----------------------------
        
        new_vocab = {"<unk>": 0}
        new_word2idx = {"<unk>": 0}
        new_idx2word = {0: "<unk>"}
        new_word_freq = {"<unk>": 0} 
        
        current_idx = 1
        for word, count in sorted(vocab.items(), key=lambda item: item[1], reverse=True):
            if word not in new_word2idx: 
                new_vocab[word] = count
                new_word2idx[word] = current_idx
                new_idx2word[current_idx] = word
                new_word_freq[word] = count
                current_idx += 1
                
        vocab = new_vocab
        word2idx = new_word2idx
        idx2word = new_idx2word
        word_freq = new_word_freq

        # ----------------------------- 3. Vocab íŒŒì¼ ì €ì¥ (pkl íŒŒì¼) -----------------------------
        vocab_data = {"vocab": vocab, "word2idx": word2idx, "idx2word": idx2word, "word_freq": word_freq}
        vocab_filename = "vocab_data_3.pkl" 
        vocab_path = os.path.join("data/pretrain", vocab_filename)
        os.makedirs(os.path.dirname(vocab_path), exist_ok=True)
        
        with open(vocab_path, "wb") as f:
            pickle.dump(vocab_data, f)
        print(f"âœ… Final Vocab (Size: {len(vocab):,}) saved to {vocab_path}")
        print(f"   <unk> ì¸ë±ìŠ¤: {word2idx['<unk>']}, ë‹¤ìŒ ë‹¨ì–´({idx2word[1]}): 1")
        
        # ----------------------------- 4. í•™ìŠµ ì¸ë±ìŠ¤ ìƒì„± ë° ì €ì¥ (npy ë°”ì´ë„ˆë¦¬ íŒŒì¼) -----------------------------
        # â­ ìµœì í™” ì ìš©: ì´ë¯¸ í† í°í™”ëœ ì„ì‹œ íŒŒì¼ì„ ë‹¤ì‹œ ì½ì–´ ìŠ¤íŠ¸ë¦¼ ìƒì„±
        final_token_stream = word_stream_generator_from_tokenized_file(TOKENIZED_TRAIN_PATH) 
        save_token_indices_to_binary(final_token_stream, word2idx, TOKEN_INDICES_PATH)
        
        # ----------------------------- 5. ì„ì‹œ íŒŒì¼ ì‚­ì œ -----------------------------
        if os.path.exists(TOKENIZED_TRAIN_PATH):
            os.remove(TOKENIZED_TRAIN_PATH) 
            print(f"ğŸ§¹ Removed temporary file: {TOKENIZED_TRAIN_PATH}")
            
    except FileNotFoundError as e:
        print(f"ì˜¤ë¥˜: {e}")
    except Exception as e:
        print(f"ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")
