import numpy as np
from scipy import stats
import pandas as pd
import os

# ===== í‰ê°€ ê²°ê³¼ ë°ì´í„° =====
evaluation_data = {
    'hs_window-2_sub-True': {
        'WordSim-353': [0.6783, 0.6747, 0.6649],
        'SimLex-999': [0.3019, 0.2940, 0.2894],
        'Google Analogy': [0.3078, 0.3116, 0.3162]
    },
    'hs_window-5_sub-True': {
        'WordSim-353': [0.6965, 0.6932, 0.7074],
        'SimLex-999': [0.2869, 0.2782, 0.2902],
        'Google Analogy': [0.3429, 0.3449, 0.3454]
    }
}

# ===== í•™ìŠµ ë©”íŠ¸ë¦­ ë°ì´í„° =====
training_metrics = {
    'hs_window-2_sub-True': {
        'loss': [0.5986, 0.5987, 0.5986],
        'duration': [22735.11, 22824.95, 22856.37],
        'gpu_memory': [3.53, 3.53, 3.53]
    },
    'hs_window-5_sub-True': {
        'loss': [0.6145, 0.6145, 0.6145],
        'duration': [45841.53, 45761.36, 45787.97],
        'gpu_memory': [3.53, 3.53, 3.53]
    },
    'ns_window-2_sub-True': {
        'loss': [2.0327, 2.0307, 2.0326],
        'duration': [17539.07, 38295.25, 17553.21],
        'gpu_memory': [2.35, 3.53, 2.35]
    },
    'ns_window-5_sub-False': {
        'loss': [2.1973, 2.2041, 2.2011],
        'duration': [37255.78, 37335.02, 37270.55],
        'gpu_memory': [2.35, 2.35, 2.35]
    }
}

def calculate_stats(values):
    """í‰ê· , í‘œì¤€í¸ì°¨ ê³„ì‚°"""
    if len(values) < 2:
        return {
            'mean': values[0] if values else 0,
            'std': 0,
            'n': len(values)
        }
    
    mean = np.mean(values)
    std = np.std(values, ddof=1)
    
    return {
        'mean': mean,
        'std': std,
        'n': len(values)
    }

# eval í´ë” ìƒì„±
os.makedirs('eval', exist_ok=True)

# ===== CSV íŒŒì¼ ìƒì„± =====

# 1. í‰ê°€ ê²°ê³¼ í†µê³„ CSV
eval_results = []
for model in evaluation_data:
    for metric in evaluation_data[model]:
        if len(evaluation_data[model][metric]) >= 3:
            stats_result = calculate_stats(evaluation_data[model][metric])
            eval_results.append({
                'Model': model,
                'Metric': metric,
                'Mean': stats_result['mean'],
                'Std': stats_result['std'],
                'N': stats_result['n'],
                'Values': str(evaluation_data[model][metric])
            })

eval_df = pd.DataFrame(eval_results)
eval_df.to_csv('eval/evaluation_statistics.csv', index=False)
print("âœ… Evaluation statistics saved to: eval/evaluation_statistics.csv")

# 2. í•™ìŠµ ë©”íŠ¸ë¦­ í†µê³„ CSV
training_results = []
for model in training_metrics:
    for metric in training_metrics[model]:
        stats_result = calculate_stats(training_metrics[model][metric])
        training_results.append({
            'Model': model,
            'Metric': metric,
            'Mean': stats_result['mean'],
            'Std': stats_result['std'],
            'N': stats_result['n'],
            'Values': str(training_metrics[model][metric])
        })

training_df = pd.DataFrame(training_results)
training_df.to_csv('eval/training_statistics.csv', index=False)
print("âœ… Training statistics saved to: eval/training_statistics.csv")

# 3. ëª¨ë¸ ë¹„êµ ê²°ê³¼ CSV
comparison_results = []

# HS ëª¨ë¸ ë¹„êµ (Window 2 vs 5)
for metric in ['WordSim-353', 'SimLex-999', 'Google Analogy']:
    values1 = evaluation_data['hs_window-2_sub-True'][metric]
    values2 = evaluation_data['hs_window-5_sub-True'][metric]
    
    t_stat, p_value = stats.ttest_rel(values1, values2)
    
    # íš¨ê³¼ í¬ê¸° (Cohen's d for paired samples)
    diff = np.array(values2) - np.array(values1)
    cohens_d = np.mean(diff) / np.std(diff, ddof=1)
    
    mean1, mean2 = np.mean(values1), np.mean(values2)
    
    # í†µê³„ì  ìœ ì˜ì„±
    if p_value < 0.001:
        significance = "***"
    elif p_value < 0.01:
        significance = "**"
    elif p_value < 0.05:
        significance = "*"
    else:
        significance = "ns"
    
    comparison_results.append({
        'Comparison': 'HS Window-2 vs Window-5',
        'Metric': metric,
        'Model1_Mean': mean1,
        'Model2_Mean': mean2,
        'Difference': mean2 - mean1,
        't_statistic': t_stat,
        'p_value': p_value,
        'cohens_d': cohens_d,
        'significance': significance,
        'better_model': 'Window-5' if mean2 > mean1 else 'Window-2'
    })

# í•™ìŠµ ë©”íŠ¸ë¦­ ë¹„êµ
for metric in ['loss', 'duration', 'gpu_memory']:
    if metric in training_metrics['hs_window-2_sub-True'] and metric in training_metrics['hs_window-5_sub-True']:
        values1 = training_metrics['hs_window-2_sub-True'][metric]
        values2 = training_metrics['hs_window-5_sub-True'][metric]
        
        t_stat, p_value = stats.ttest_rel(values1, values2)
        
        mean1, mean2 = np.mean(values1), np.mean(values2)
        
        if p_value < 0.05:
            significance = "*"
        else:
            significance = "ns"
        
        comparison_results.append({
            'Comparison': 'HS Window-2 vs Window-5 (Training)',
            'Metric': metric,
            'Model1_Mean': mean1,
            'Model2_Mean': mean2,
            'Difference': mean2 - mean1,
            't_statistic': t_stat,
            'p_value': p_value,
            'cohens_d': np.nan,  # í•™ìŠµ ë©”íŠ¸ë¦­ì€ íš¨ê³¼ í¬ê¸° ê³„ì‚° ìƒëµ
            'significance': significance,
            'better_model': 'Window-2' if (metric == 'loss' and mean1 < mean2) or 
                           (metric == 'duration' and mean1 < mean2) else 'Window-5'
        })

comparison_df = pd.DataFrame(comparison_results)
comparison_df.to_csv('eval/model_comparisons.csv', index=False)
print("âœ… Model comparisons saved to: eval/model_comparisons.csv")

# 4. ìš”ì•½ í…Œì´ë¸” CSV
summary_data = []
for metric in ['WordSim-353', 'SimLex-999', 'Google Analogy']:
    values1 = evaluation_data['hs_window-2_sub-True'][metric]
    values2 = evaluation_data['hs_window-5_sub-True'][metric]
    
    mean1, mean2 = np.mean(values1), np.mean(values2)
    std1, std2 = np.std(values1, ddof=1), np.std(values2, ddof=1)
    t_stat, p_value = stats.ttest_rel(values1, values2)
    
    summary_data.append({
        'Metric': metric,
        'HS_Window2_Mean': mean1,
        'HS_Window2_Std': std1,
        'HS_Window5_Mean': mean2,
        'HS_Window5_Std': std2,
        'p_value': p_value,
        'Significant': 'Yes' if p_value < 0.05 else 'No'
    })

summary_df = pd.DataFrame(summary_data)
summary_df.to_csv('eval/summary_table.csv', index=False)
print("âœ… Summary table saved to: eval/summary_table.csv")

# 5. íš¨ìœ¨ì„± ë¶„ì„ CSV
models = ['hs_window-2_sub-True', 'hs_window-5_sub-True', 'ns_window-2_sub-True', 'ns_window-5_sub-False']
performance_wordsim = [0.6726, 0.6990, 0.6428, 0.6392]  # WordSim-353 í‰ê·  ì„±ëŠ¥
gpu_hours = [6.32, 12.72, 10.65, 10.35]  # duration í‰ê· ì„ ì‹œê°„ìœ¼ë¡œ ë³€í™˜

efficiency_data = []
for i, model in enumerate(models):
    if model in training_metrics:
        duration_mean = np.mean(training_metrics[model]['duration'])
        memory_mean = np.mean(training_metrics[model]['gpu_memory'])
        
        efficiency_data.append({
            'Model': model,
            'Performance_WordSim353': performance_wordsim[i],
            'GPU_Hours': duration_mean / 3600,
            'Memory_GB': memory_mean,
            'Performance_per_Hour': performance_wordsim[i] / (duration_mean / 3600),
            'Performance_per_GB': performance_wordsim[i] / memory_mean
        })

efficiency_df = pd.DataFrame(efficiency_data)
efficiency_df.to_csv('eval/efficiency_analysis.csv', index=False)
print("âœ… Efficiency analysis saved to: eval/efficiency_analysis.csv")

print("\nğŸ“ All CSV files saved in the 'eval/' directory:")
print("  - evaluation_statistics.csv")
print("  - training_statistics.csv") 
print("  - model_comparisons.csv")
print("  - summary_table.csv")
print("  - efficiency_analysis.csv")
