import os
import pickle
import re
from typing import Generator, List
from collections import Counter

import numpy as np
from data import build_vocab_stream, TOKEN_INDICES_PATH, TOKENIZED_TRAIN_PATH, word_stream_generator
import nltk
try:
    nltk.download("stopwords", quiet=True)
    nltk.download("punkt", quiet=True) 
    from nltk.tokenize import word_tokenize
    from nltk.corpus import stopwords
except ImportError:
    print("NLTK is not installed. Using simple split() for tokenization.")
    def word_tokenize(text):
        return re.findall(r"\b\w+\b", text) 
    stopwords = set()

# -----------------------------
CORPUS_PATH = "data/pretrain/word2vec_corpus_hf_half.txt"
# TOKENIZED_SAVE_PATH = "data/pretrain/tokenized_corpus.txt"

STOPWORDS = set(stopwords.words('english')) if 'stopwords' in locals() and stopwords else set()

# -----------------------------
def clean_token(token: str):
    """ì „ì²˜ë¦¬: URL, ìˆ«ì ì œê±°, íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬, ì†Œë¬¸ìí™”, ë¶ˆìš©ì–´ ì œê±°"""
    token = token.lower()

    token = re.sub(r'\d+', '', token)
    token = re.sub(r"[^a-z'-]", '', token)
    
    if not token or token in STOPWORDS or token.strip() == '' or len(token) < 2:
        return None
    return token

def preprocess_tokens(tokens: list):
    """í† í° ë¦¬ìŠ¤íŠ¸ ì „ì²´ ì „ì²˜ë¦¬"""
    cleaned_tokens = []
    for t in tokens:
        ct = clean_token(t)
        if ct:
            cleaned_tokens.append(ct)
    return cleaned_tokens

# -----------------------------
def preprocess_text(text: str) -> list:
    """ë‹¨ì¼ ë¬¸ì„œ í…ìŠ¤íŠ¸ì— ëŒ€í•´ ì „ì²˜ë¦¬ ë° í† í°í™”ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤."""

    text = re.sub(r'==\s*(References|External links|See also|Notes|Sources)\s*==.*', '', text, flags=re.DOTALL | re.IGNORECASE)
    
    text = re.sub(r'https?://\S+|www\.\S+', '', text)
    
    tokens = word_tokenize(text)

    return preprocess_tokens(tokens)

# -----------------------------
def process_corpus_and_stream(path=CORPUS_PATH) -> Generator[List[str], None, None]:
    """
    ì›ë³¸ íŒŒì¼ì„ í•œ ì¤„ì”© ì½ì–´ ë¬¸ì„œ('\n\n'ìœ¼ë¡œ êµ¬ë¶„)ë¥¼ ì¬êµ¬ì„±í•˜ê³ , 
    ì „ì²˜ë¦¬ ë° í† í°í™”ëœ í† í° ë¦¬ìŠ¤íŠ¸(ë¬¸ì¥/ë¬¸ë§¥ ë‹¨ìœ„)ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ yield
    """
    if not os.path.exists(path):
        raise FileNotFoundError(f"Corpus file not found: {path}.")
    
    print(f"Starting streaming process from {path}.")
    
    doc_buffer = []
    doc_count = 0
    
    with open(path, "r", encoding="utf-8", errors="ignore") as f:
        for line in f:
            line = line.strip()
            
            if line:
                doc_buffer.append(line)
                
            if not line and doc_buffer:
                doce_text = " ".join(doc_buffer)
                
                tokens = preprocess_text(doce_text)
                
                if tokens:
                    yield tokens 
                    doc_count += 1
                    
                doc_buffer = []
                
                if doc_count % 100000 == 0 and doc_count > 0:
                    print(f"Processed {doc_count:,} documents so far...")
                    
        if doc_buffer:
            doce_text = " ".join(doc_buffer)
            tokens = preprocess_text(doce_text)
            if tokens:
                yield tokens 
                doc_count += 1
    print(f"\nProcessing complete. Total documents processd: {doc_count:,}")


def save_token_indices_to_binary(
    token_stream: Generator[List[str], None, None],
    word2idx: dict,
    save_path=TOKEN_INDICES_PATH
):
    print(f"Indexing corpus and saving to {save_path}...")

    all_indices = []
    total_tokens_count = 0
    
    for tokens in token_stream:
        indices = [word2idx[token] for token in tokens if token in word2idx]
        all_indices.extend(indices)
        
        total_tokens_count += len(indices)
        if total_tokens_count % 50000000 == 0 and total_tokens_count > 0:
            print(f"Tokens indexed so far: {total_tokens_count:,}")
    
    token_indices_array = np.array(all_indices, dtype=np.int32)
    
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    np.save(save_path, token_indices_array)
    
    print(f"\nâœ… Corpus indexing complete. Total indices: {len(token_indices_array):,}. Saved to {save_path}")

if __name__ == "__main__":
    
    # ----------------------------- 1. Vocab êµ¬ì¶• ë° ì„ì‹œ íŒŒì¼ ìƒì„± (NS/HS ê³µí†µ) -----------------------------
    try:
        # A. ì›ë³¸ ì½”í¼ìŠ¤ë¥¼ ì½ì–´ í† í°í™”ëœ ë‚´ìš©ì„ ì„ì‹œ íŒŒì¼ì— ì €ì¥ (Vocab êµ¬ì¶•ìš©)
        print(f"Saving temporary tokenized corpus to {TOKENIZED_TRAIN_PATH} for vocab building...")
        total_temp_tokens = 0
        os.makedirs(os.path.dirname(TOKENIZED_TRAIN_PATH), exist_ok=True)
        with open(TOKENIZED_TRAIN_PATH, "w", encoding="utf-8") as f:
            temp_stream = process_corpus_and_stream(CORPUS_PATH)
            for tokens in temp_stream:
                f.write(" ".join(tokens) + "\n")
                total_temp_tokens += len(tokens)
        print(f"Temporary tokenized file created. Total tokens: {total_temp_tokens}")
        
        # B. ì„ì‹œ íŒŒì¼ë¡œ Vocab êµ¬ì¶•
        VOCAB_MIN_COUNT = 50 # config ê°’ì„ ê°€ì •
        vocab, word2idx, idx2word, word_freq = build_vocab_stream(
            TOKENIZED_TRAIN_PATH,
            min_count=VOCAB_MIN_COUNT
        )
        
        # ----------------------------- 2. Vocab íŒŒì¼ ì €ì¥ (NS/HS ê³µí†µ) -----------------------------
        # Vocab íŒŒì¼ ì €ì¥: train.pyê°€ ë¡œë“œí•  ìˆ˜ ìˆë„ë¡ ì €ì¥í•©ë‹ˆë‹¤. (ì—¬ê¸°ì„œëŠ” NS/HS ê³µí†µìœ¼ë¡œ ì‚¬ìš©í•œë‹¤ê³  ê°€ì •)
        vocab_data = {"vocab": vocab, "word2idx": word2idx, "idx2word": idx2word, "word_freq": word_freq}
        vocab_filename = "vocab_data_3.pkl"  # train.pyì™€ ë‹¤ë¥¸ ìŠ¤í¬ë¦½íŠ¸ë“¤ì´ ê¸°ëŒ€í•˜ëŠ” íŒŒì¼ëª…
        vocab_path = os.path.join("data/pretrain", vocab_filename)
        os.makedirs(os.path.dirname(vocab_path), exist_ok=True)
        
        with open(vocab_path, "wb") as f:
            pickle.dump(vocab_data, f)
        print(f"âœ… Final Vocab saved to {vocab_path}")
        
        # ----------------------------- 3. í•™ìŠµ ì¸ë±ìŠ¤ ìƒì„± ë° ì €ì¥ -----------------------------
        # Vocab êµ¬ì¶•ì„ ìœ„í•´ ì‚¬ìš©í•œ ìŠ¤íŠ¸ë¦¼ì€ ì†Œì§„ë˜ì—ˆìœ¼ë¯€ë¡œ, ìƒˆ ìŠ¤íŠ¸ë¦¼ ìƒì„±
        final_token_stream = process_corpus_and_stream(CORPUS_PATH) 
        save_token_indices_to_binary(final_token_stream, word2idx, TOKEN_INDICES_PATH)
        
        # ----------------------------- 4. ì„ì‹œ íŒŒì¼ ì‚­ì œ (ìœ ì§€) -----------------------------
        if os.path.exists(TOKENIZED_TRAIN_PATH):
            os.remove(TOKENIZED_TRAIN_PATH) # ğŸ‘ˆ ì´ íŒŒì¼ì€ ì´ì œ í•„ìš” ì—†ìœ¼ë¯€ë¡œ ì‚­ì œ
            print(f"ğŸ§¹ Removed temporary file: {TOKENIZED_TRAIN_PATH}")
            
    except FileNotFoundError as e:
        print(f"ì˜¤ë¥˜: {e}")
    except Exception as e:
        print(f"ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")