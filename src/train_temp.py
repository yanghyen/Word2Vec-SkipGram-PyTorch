import torch
import torch.nn as nn
import torch.optim as optim
import yaml, argparse, random, os, pickle
import numpy as np
from tqdm import tqdm
import time
import csv # [ì¶”ê°€] CSV íŒŒì¼ ì €ì¥ì„ ìœ„í•´ csv ëª¨ë“ˆ import

from data import build_vocab_stream, get_dataloader, TOKENIZED_TRAIN_PATH
from model import SkipGram
from huffman_tree import HuffmanTree  

import os
os.environ['CUDA_LAUNCH_BLOCKING'] = '1'
TOKEN_INDICES_PATH = "data/pretrain/token_indices.npy"

parser = argparse.ArgumentParser()
parser.add_argument("--config", type=str, default="configs/ns_window-2_epoch-5.yaml")
args = parser.parse_args()

with open(args.config, "r") as f:
    config = yaml.safe_load(f)

def set_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

set_seed(config["seed"])

# [ìˆ˜ì •] í•™ìŠµ ì‹œì‘ ì‹œê°„ ê¸°ë¡
start_time = time.time() 

# text = load_tokenized_corpus_from_file()

# vocab, word2idx, idx2word = build_vocab(text)
# text = [token for token in text if token in word2idx]
# text = subsample_text(text, t=1e-4)

vocab, word2idx, idx2word, word_freq = build_vocab_stream(
    TOKENIZED_TRAIN_PATH, 
    min_count=config.get("min_count", 5) 
)
vocab_size = len(vocab)
# dataloader = get_dataloader(text, config, word2idx, vocab, mode=config["training_mode"])

dataloader = get_dataloader(
    TOKEN_INDICES_PATH, 
    config, 
    word2idx, 
    word_freq, # ğŸ‘ˆ ë¹ˆë„ìˆ˜ ì „ë‹¬
    mode=config["training_mode"]
)

embedding_dim = config["embedding_dim"]
model = SkipGram(vocab_size, embedding_dim)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

if device.type == 'cuda':
    torch.cuda.reset_max_memory_allocated()
    print(f"ğŸ”¥ Device: {device}, Initial GPU Memory: {torch.cuda.memory_allocated(device) / 1024**3:.2f} GB")

optimizer = optim.Adam(model.parameters(), lr=config["lr"])

if config["training_mode"] == "hs":
    print("ğŸ§© Building Huffman Tree...")
    word_freq = {w: vocab[w] for w in vocab}
    huffman_tree = HuffmanTree(word_freq)
    path_table = {}
    code_table = {}
    for word, idx in word2idx.items():
        path_table[idx] = huffman_tree.get_path(idx)
        code_table[idx] = huffman_tree.get_code(idx)

num_epochs = config["epochs"]
checkpoint_dir = f"runs/checkpoints_{config['training_mode']}"
os.makedirs(checkpoint_dir, exist_ok=True)
try:
    total_batches = len(dataloader) 
except TypeError:
    print("âš ï¸ Warning: IterableDataset has no definite length. Using estimated steps.")
    # ì‹¤ì œ í† í° ìˆ˜ ê¸°ë°˜ìœ¼ë¡œ ëŒ€ëµì ì¸ ë°°ì¹˜ë¥¼ ê³„ì‚°í•´ì•¼ í•¨.
    total_batches = 500000
    
total_steps = config["epochs"] * total_batches
current_step = 0
print(f"ğŸš€ Training SkipGram ({config['training_mode'].upper()}) mode...")

# [ì¶”ê°€] ì—í­ë³„ í•™ìŠµ ì§€í‘œë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
metrics_log = [] 

for epoch in range(1, num_epochs + 1):
    epoch_start_time = time.time()
    total_loss = 0
    model.train()
    
    # estimated_steps = 800  
    progress_bar = tqdm(
        dataloader,
        total=total_batches,
        desc=f"Epoch {epoch}/{num_epochs}",
        dynamic_ncols=True
    )
    
    batch_count_in_epoch = 0

    # for batch in tqdm(dataloader, desc=f"Epoch {epoch}/{num_epochs}"):
    for batch in progress_bar:
        current_step += 1
        batch_count_in_epoch += 1
        
        progress = current_step / total_steps 
        new_lr = config["lr"] * (1 - progress) 
        new_lr = max(0.0001, new_lr) 
        
        for param_group in optimizer.param_groups:
            param_group['lr'] = new_lr

        optimizer.zero_grad()

        if config["training_mode"] == "ns":
            center, pos_context, neg_samples = batch
            loss = model.forward_ns(center.to(device), pos_context.to(device), neg_samples.to(device))

        elif config["training_mode"] == "hs":
            # center_idx, target_idx = batch
            # path_idx = [path_table[t.item()] for t in target_idx]
            # code_tensor = [code_table[t.item()] for t in target_idx]
            # loss = model.forward_hs(center_idx.to(device), path_idx, code_tensor)
            raise NotImplementedError("HS streaming mode is not yet fully implemented.")


        loss.backward()
        optimizer.step()
        total_loss += loss.item()
        
        avg_loss = total_loss / batch_count_in_epoch
        progress_bar.set_postfix({
            "loss": f"{loss.item():.4f}",
            "avg_loss": f"{avg_loss:.4f}",
            "lr": f"{new_lr:.6f}"
        })

    progress_bar.close()


    epoch_end_time = time.time()
    epoch_duration = epoch_end_time - epoch_start_time
    
    # avg_loss = total_loss / len(dataloader)
    avg_loss = total_loss / batch_count_in_epoch
    
    max_memory = 0
    if device.type == 'cuda':
        max_memory = torch.cuda.max_memory_allocated(device) / 1024**3
        torch.cuda.reset_max_memory_allocated() # ë‹¤ìŒ ì—í­ ì¸¡ì •ì„ ìœ„í•´ ë¦¬ì…‹
        print(f"Epoch {epoch}/{num_epochs}, Loss: {avg_loss:.4f}, Time: {epoch_duration:.2f}s, Max GPU Memory: {max_memory:.2f} GB")
    else:
        print(f"Epoch {epoch}/{num_epochs}, Loss: {avg_loss:.4f}, Time: {epoch_duration:.2f}s")
    
    
    # [ì¶”ê°€] ì—í­ë³„ ì§€í‘œë¥¼ ë¡œê·¸ ë¦¬ìŠ¤íŠ¸ì— ì €ì¥
    metrics_log.append({
        "epoch": epoch,
        "loss": avg_loss,
        "duration_seconds": epoch_duration,
        "max_gpu_memory_gb": max_memory,
    })

    ckpt_path = os.path.join(checkpoint_dir, f"{config['training_mode']}_window-{config['window_size']}_epoch-{epoch}.pth")
    
    torch.save({
        "epoch": epoch,
        "model_state_dict": model.state_dict(),
        "optimizer_state_dict": optimizer.state_dict(),
        "loss": avg_loss,
        "config": config,
        "epoch_duration": epoch_duration,
        "max_gpu_memory": max_memory if device.type == 'cuda' else None,
    }, ckpt_path)
    print(f"Saved checkpoint to {ckpt_path}")

# [ìˆ˜ì •] ì „ì²´ í•™ìŠµ ì‹œê°„ ê¸°ë¡ ë° CSV ì €ì¥
end_time = time.time()
total_training_duration = end_time - start_time
print(f"\nâœ… Total Training Time: {total_training_duration:.2f} seconds ({total_training_duration / 3600:.2f} GPU-hours)")

# ----------------- CSV ì €ì¥ ë¡œì§ -----------------
# [ì¶”ê°€] CSV íŒŒì¼ ì´ë¦„ ì„¤ì • ë° ì €ì¥ ê²½ë¡œ ìƒì„±
csv_filename = f"metrics_{config['training_mode']}_window-{config['window_size']}_seed-{config['seed']}.csv"
csv_path = os.path.join("runs/results", csv_filename)

if metrics_log:
    # í—¤ë” ì •ì˜
    fieldnames = ["epoch", "loss", "duration_seconds", "max_gpu_memory_gb"]
    
    # runs í´ë”ê°€ ì—†ìœ¼ë©´ ìƒì„±
    os.makedirs("runs/results", exist_ok=True)
    
    with open(csv_path, 'w', newline='') as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        
        writer.writeheader()
        writer.writerows(metrics_log)

    print(f"ğŸ“Š Metrics saved to {csv_path}")
else:
    print("âš ï¸ Warning: Metrics log is empty. CSV file not created.")
# -------------------------------------------------

vocab_data = {"vocab": vocab, "word2idx": word2idx, "idx2word": idx2word}
training_mode = config.get("training_mode", "default")
vocab_filename = f"vocab_{training_mode}.pkl"
vocab_path = os.path.join("runs", vocab_filename)

with open(vocab_path, "wb") as f:
    pickle.dump(vocab_data, f)

print("Training finished successfully!")