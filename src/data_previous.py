# src/data.py
from collections import Counter
import random
import csv
import torch
from torch.utils.data import Dataset, DataLoader, IterableDataset
import os
import pickle
from typing import Generator, List

import numpy as np

import xml.etree.ElementTree as ET

def subsample_text(text, t=1e-4): 
    counter = Counter(text)
    total_count = len(text)
    freqs = {word: count / total_count for word, count in counter.items()}
    
    subsampled = []
    for word in text:
        f = freqs[word]
        p_drop = 1 - ((t / f) ** 0.5)
        p_drop = max(0, p_drop) 
        
        if random.random() > p_drop: 
            subsampled.append(word)
    return subsampled

# def load_tokenized_corpus_from_file(path=TOKENIZED_SAVE_PATH) -> list:
#     if not os.path.exists(path):
#         raise FileNotFoundError(f"Tokenized corpus file not found: {path}.")
#     print(f"Loading tokenized corpus from saved file: {path}...")
    
#     try:
#         with open(path, 'rb') as f:
#             tokens = pickle.load(f)
        
#         if not isinstance(tokens, list):
#             raise TypeError("Loaded object is not a list. Check the data format in the .pkl file.")

#         print(f"Tokenized corpus successfully loaded. Total tokens: {len(tokens)}")
#         return tokens
        
#     except Exception as e:
#         print(f"Error loading pickle file: {e}")
#         raise
TOKENIZED_TRAIN_PATH = "data/pretrain/tokenized_corpus.txt"
TOKEN_INDICES_PATH = "data/pretrain/token_indices.npy"

def word_stream_generator(file_path) -> Generator[List[str], None, None]:
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"Tokenized corpus file not found: {file_path}")
    
    with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
        for line in f:
            for token in line.strip().split():
                yield token
                
def build_vocab_stream(file_path, min_count=10):
    vocab_counter = Counter()
    print(f"Building vocab from stream: {file_path} (min_counter={min_count})")
    
    for token in word_stream_generator(file_path):
        vocab_counter[token] += 1
        
    vocab = {word: count for word, count in vocab_counter.items() if count >= min_count}
    word2idx = {word: i for i, word in enumerate(vocab.keys())}
    idx2word = {i: word for word, i in word2idx.items()}
    
    print(f"Built vocab: {len(vocab)} words (min_count={min_count})")
    word_freq = {word: vocab_counter[word] for word in vocab}
    
    return vocab, word2idx, idx2word, word_freq

def load_wordsim353(path):
    pairs = []
    with open(path, 'r', encoding='utf-8') as f:
        reader = csv.reader(f)
        next(reader)  
        for row in reader:
            w1, w2, sim = row[0].lower(), row[1].lower(), float(row[2])
            pairs.append((w1, w2, sim))
    return pairs

def load_simlex999(path):
    pairs = []
    with open(path, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f, delimiter='\t')
        for row in reader:
            w1, w2, sim = row['word1'].lower(), row['word2'].lower(), float(row['SimLex999'])
            pairs.append((w1, w2, sim))
    return pairs

def load_google_analogy(path):
    analogies = []
    with open(path, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if line.startswith(':') or len(line) == 0:
                continue
            words = line.lower().split()
            if len(words) == 4:
                analogies.append(tuple(words))
    return analogies

# def build_vocab(text, min_count=5):

#     tokens = text if isinstance(text, list) else text.lower().split()
#     vocab_counter = Counter(tokens)
#     vocab = {word: count for word, count in vocab_counter.items() if count >= min_count}

#     word2idx = {word: i for i, word in enumerate(vocab.keys())}
#     idx2word = {i: word for word, i in word2idx.items()}
    
#     print(f"ğŸ“š Built vocab: {len(vocab)} words (min_count={min_count})")
    
#     return vocab, word2idx, idx2word
class SkipGramNSIterableDataset(IterableDataset):
    def __init__(self, file_path, word2idx, word_freq, neg_sample_size=5, window_size=2, subsample_t=1e-4):
        super().__init__()
        self.file_path = file_path
        self.word2idx = word2idx
        self.window_size = window_size
        self.neg_sample_size = neg_sample_size
        self.subsample_t = subsample_t
        self.vocab_size = len(word2idx)
        self.idx2word = {i: w for w, i in word2idx.items()} # í¸ì˜ë¥¼ ìœ„í•´ ì¶”ê°€
        
        # Negative Sampling í™•ë¥  ê³„ì‚°: min_countë¥¼ í†µê³¼í•œ ë‹¨ì–´ë“¤ì˜ ë¹ˆë„ ì‚¬ìš©
        freqs_list = [word_freq.get(self.idx2word.get(i), 0) for i in range(self.vocab_size)] 
        self.freqs_for_neg = torch.tensor(freqs_list, dtype=torch.float)
        self.freqs_for_neg[self.freqs_for_neg == 0] = 1e-30 # 0 ë°©ì§€
        self.sample_probs = self.freqs_for_neg.pow(0.75) / self.freqs_for_neg.pow(0.75).sum()
        self.sample_probs = self.sample_probs.to("cuda")
        
        # Subsampling í™•ë¥  ê³„ì‚°
        self.total_count = sum(word_freq.values())
        self.freqs = {word: count / self.total_count for word, count in word_freq.items()}
        
        if not os.path.exists(self.file_path):
            raise FileNotFoundError(f"Binary index file not found: {self.file_path}. Run preprocessing first!")
        
        try:
            self.token_indices = np.load(self.file_path, mmap_mode='r')
            self.total_tokens = len(self.token_indices)
            print(f"Loaded token indices (mmap): Total tokens = {self.total_tokens:,}")
        except Exception as e:
            raise RuntimeError(f"Error loading token indices file via mmap: {e}")
        
        freqs_list = [word_freq.get(self.idx2word.get(i), 0) for i in range(self.vocab_size)] 
        self.freqs_for_neg = torch.tensor(freqs_list, dtype=torch.float)
        self.freqs_for_neg[self.freqs_for_neg == 0] = 1e-30 # 0 ë°©ì§€
        self.sample_probs = self.freqs_for_neg.pow(0.75) / self.freqs_for_neg.pow(0.75).sum()
        
        # Subsampling í™•ë¥  ê³„ì‚°
        self.total_count = sum(word_freq.values())
        self.freqs = {word: count / self.total_count for word, count in word_freq.items()}
        # ìœˆë„ìš° í¬ê¸° + 1 ë§Œí¼ ë²„í¼ë¥¼ ìœ ì§€í•˜ì—¬ context windowë¥¼ ë§Œë“­ë‹ˆë‹¤.
        # ì‹¤ì œ Word2Vec í•™ìŠµì€ í† í° ì¸ë±ìŠ¤ ë‹¨ìœ„ë¡œ ì´ë£¨ì–´ì§‘ë‹ˆë‹¤. 
        # í† í° ì¸ë±ìŠ¤ë¡œ ë³€í™˜ëœ ì‹œí€€ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ë²„í¼ë¥¼ ê´€ë¦¬í•©ë‹ˆë‹¤.

    # def _line_to_index_stream(self):
    #     """íŒŒì¼ì„ í•œ ì¤„ì”© ì½ì–´ ì¸ë±ìŠ¤ë¡œ ë³€í™˜í•˜ê³ , ìœ íš¨í•œ ë‹¨ì–´ë§Œ ë‚¨ê¸°ëŠ” ì œë„ˆë ˆì´í„°."""
    #     with open(self.file_path, 'r', encoding='utf-8', errors='ignore') as f:
    #         for line in f:
    #             indices = [self.word2idx[token] for token in line.strip().split() if token in self.word2idx]
    #             yield indices

    # def __iter__(self):
    #     """í•™ìŠµ ìŒ (center, context, neg_samples)ì„ yield"""
        
    #     # Word2Vec í•™ìŠµì„ ìœ„í•œ Sliding Window ë²„í¼
    #     buffer = [] 
        
    #     # ë¼ì¸(ë¬¸ë§¥ ë‹¨ìœ„) ì¸ë±ìŠ¤ ìŠ¤íŠ¸ë¦¼ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
    #     index_stream = self._line_to_index_stream()

    #     for indices in index_stream:
    #         # ìƒˆ ë¬¸ë§¥ì„ ë²„í¼ì— ì¶”ê°€í•©ë‹ˆë‹¤.
    #         buffer.extend(indices)
            
    #         # ë²„í¼ê°€ window_sizeë¥¼ ì´ˆê³¼í•˜ëŠ” ë™ì•ˆ ë°˜ë³µ
    #         while len(buffer) > 0:
    #             # 1. ì¤‘ì‹¬ ë‹¨ì–´ ì„¤ì • ë° ì„œë¸Œìƒ˜í”Œë§
    #             center_idx = buffer.pop(0) # ê°€ì¥ ì˜¤ë˜ëœ í† í°ì„ ì¤‘ì‹¬ ë‹¨ì–´ë¡œ
    #             center_token = self.idx2word[center_idx]
                
    #             # ì„œë¸Œìƒ˜í”Œë§ í™•ë¥ 
    #             f = self.freqs.get(center_token, 0)
    #             p_drop = 1 - ((self.subsample_t / f) ** 0.5) if f > 0 else 1
    #             if random.random() < p_drop: 
    #                 continue # ë“œë¡­

    #             # 2. ê°€ë³€ ìœˆë„ìš° ì„¤ì •
    #             actual_window = random.randint(1, self.window_size)
                
    #             # 3. ì£¼ë³€ ë‹¨ì–´(Context) ì„ íƒ
    #             # í˜„ì¬ ë²„í¼(ì¤‘ì‹¬ë‹¨ì–´ ì´í›„)ì™€ ë²„í¼ ì•(ì¤‘ì‹¬ë‹¨ì–´ ì´ì „, ì¦‰ popëœ í† í°)ì˜ í† í°ì„ ëª¨ë‘ ê³ ë ¤í•´ì•¼ í•˜ì§€ë§Œ,
    #             # IterableDatasetì˜ ìŠ¤íŠ¸ë¦¬ë° íŠ¹ì„±ìƒ ì´ì „ í† í°ì€ ì¬êµ¬ì„±ì´ ì–´ë µìŠµë‹ˆë‹¤.
    #             # ì—¬ê¸°ì„œëŠ” *í˜„ì¬ ë²„í¼ì˜ í† í°*ì„ ì£¼ë³€ ë‹¨ì–´ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
                
    #             context_indices = buffer[:actual_window] # ì˜¤ë¥¸ìª½ ë¬¸ë§¥ë§Œ ê³ ë ¤ (ê°„ì†Œí™”)
                
    #             for context_idx in context_indices:
                    
    #                 # 4. ë„¤ê±°í‹°ë¸Œ ìƒ˜í”Œë§
    #                 neg_samples = torch.multinomial(
    #                     self.sample_probs,
    #                     self.neg_sample_size,
    #                     replacement=True
    #                 )
                    
    #                 # 5. í•™ìŠµ ìŒ Yield
    #                 yield torch.tensor(center_idx, dtype=torch.long), \
    #                       torch.tensor(context_idx, dtype=torch.long), \
    #                       neg_samples   
    
    def __iter__(self):
        """í•™ìŠµ ìŒ (center, context, neg_samples)ì„ yield"""
        
        # ğŸŸ¢ [ìˆ˜ì •] ì›Œì»¤ë³„ ë°ì´í„° ë¶„í•  (mmap ë°°ì—´ ì¸ë±ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ë³‘ë ¬ ì²˜ë¦¬)
        worker_info = torch.utils.data.get_worker_info()
        if worker_info is None:
            start_idx = 0
            end_idx = self.total_tokens
        else:
            per_worker = self.total_tokens // worker_info.num_workers
            start_idx = worker_info.id * per_worker
            end_idx = start_idx + per_worker
            if worker_info.id == worker_info.num_workers - 1:
                 end_idx = self.total_tokens # ë§ˆì§€ë§‰ ì›Œì»¤ëŠ” ë‚˜ë¨¸ì§€ë¥¼ ëª¨ë‘ ì²˜ë¦¬
                 
        current_idx = start_idx
        
        while current_idx < end_idx:
            # 1. ì¤‘ì‹¬ ë‹¨ì–´ ì„¤ì • ë° ì„œë¸Œìƒ˜í”Œë§
            center_idx = self.token_indices[current_idx] # mmap ë°°ì—´ì—ì„œ ì¸ë±ìŠ¤ ì ‘ê·¼
            center_token = self.idx2word[center_idx]
            
            # ì„œë¸Œìƒ˜í”Œë§ í™•ë¥ 
            f = self.freqs.get(center_token, 0)
            p_drop = 1 - ((self.subsample_t / f) ** 0.5) if f > 0 else 1
            if random.random() < p_drop: 
                current_idx += 1 # ë“œë¡­ëœ ê²½ìš°ì—ë„ ì¸ë±ìŠ¤ ì¦ê°€
                continue 

            # 2. ê°€ë³€ ìœˆë„ìš° ì„¤ì •
            actual_window = random.randint(1, self.window_size)
            
            # 3. ì£¼ë³€ ë‹¨ì–´(Context) ì„ íƒ
            # ë¬¸ë§¥ ì¸ë±ìŠ¤ ë²”ìœ„ ê³„ì‚° (í˜„ì¬ ì›Œì»¤ì˜ ë²”ìœ„(start_idx, end_idx)ë¥¼ ë²—ì–´ë‚˜ì§€ ì•Šë„ë¡ ì œí•œ)
            left_context_start = max(start_idx, current_idx - actual_window)
            right_context_end = min(end_idx, current_idx + actual_window + 1)
            
            for context_token_idx in range(left_context_start, right_context_end):
                if context_token_idx == current_idx:
                    continue
                    
                context_idx = self.token_indices[context_token_idx]

                # 4. ë„¤ê±°í‹°ë¸Œ ìƒ˜í”Œë§ (ë³€ê²½ ì—†ìŒ)
                neg_samples = torch.multinomial(
                    self.sample_probs,
                    self.neg_sample_size,
                    replacement=True
                )
                
                # 5. í•™ìŠµ ìŒ Yield
                yield torch.tensor(center_idx, dtype=torch.long), \
                      torch.tensor(context_idx, dtype=torch.long), \
                      neg_samples
                      
            current_idx += 1 # ì¤‘ì‹¬ ë‹¨ì–´ ì¸ë±ìŠ¤ ì¦ê°€ 
                          
                          
class SkipGramNSDataset(Dataset):
    def __init__(self, tokens, word2idx, vocab, neg_sample_size=5, window_size=2):
        
        self.tokens = tokens  
        self.word2idx = word2idx
        self.window_size = window_size
        self.neg_sample_size = neg_sample_size
        
        token_counter = Counter(self.tokens)
        
        idx_to_word_list = sorted(word2idx.items(), key=lambda item: item[1])
        
        freqs_list = []
        for word, idx in idx_to_word_list:
            freqs_list.append(token_counter.get(word, 0))
            
        freqs = torch.tensor(freqs_list, dtype=torch.float)
        
        freqs[freqs == 0] = 1e-30
        
        self.sample_probs = freqs.pow(0.75) / freqs.pow(0.75).sum()
        
    def __len__(self):
        return len(self.tokens)

    def __getitem__(self, idx):
        max_retries = 10
        for _ in range(max_retries):
            try:
                center_token = self.tokens[idx]
                center_idx = torch.tensor(self.word2idx[center_token], dtype=torch.long)

                window = random.randint(1, self.window_size)
                start = max(0, idx - window)
                end = min(len(self.tokens), idx + window + 1)
                
                context_candidates = list(range(start, end))
                if idx in context_candidates:
                    context_candidates.remove(idx)

                if not context_candidates:
                    j = random.choice([k for k in range(max(0, idx - 1), min(len(self.tokens), idx + 2)) if k != idx])
                else:
                    j = random.choice(context_candidates)

                context_token = self.tokens[j]
                context_idx = torch.tensor(self.word2idx[context_token], dtype=torch.long)

                neg_samples = torch.multinomial(
                    self.sample_probs,
                    self.neg_sample_size,
                    replacement=True
                )
                
                return center_idx, context_idx, neg_samples
                
            except KeyError:
                idx = random.randint(0, len(self.tokens) - 1)
                continue
                
        raise RuntimeError("Failed to sample a valid token after multiple retries.")

class SkipGramHSDataset(Dataset):
    def __init__(self, tokens, word2idx, window_size=2):
        self.tokens = tokens  
        self.word2idx = word2idx
        self.window_size = window_size

    def __len__(self):
        return len(self.tokens)

    def __getitem__(self, idx):
        center_token = self.tokens[idx]
        center_idx = torch.tensor(self.word2idx[center_token], dtype=torch.long)

        window = random.randint(1, self.window_size)
        start = max(0, idx - window)
        end = min(len(self.tokens), idx + window + 1)
        
        context_candidates = list(range(start, end))
        if idx in context_candidates:
          context_candidates.remove(idx)
        
        j = random.choice(context_candidates) if context_candidates else idx
        
        target_token = self.tokens[j]
        target_idx = torch.tensor(self.word2idx[target_token], dtype=torch.long)
        
        return center_idx, target_idx

# def get_dataloader(text, config, word2idx, vocab=None, mode="ns"):
#     tokens = text if isinstance(text, list) else text.lower().split()

#     if mode == "ns":
#         dataset = SkipGramNSDataset(
#             tokens=tokens, 
#             word2idx=word2idx, 
#             vocab=vocab, 
#             neg_sample_size=config.get("neg_sample_size", 5),
#             window_size=config["window_size"]
#         )
#     elif mode == "hs":
#         dataset = SkipGramHSDataset(
#             tokens, 
#             word2idx,
#             window_size=config["window_size"]
#         )
#     else:
#         raise ValueError("mode must be 'ns' or 'hs'")

#     dataloader = DataLoader(
#         dataset,
#         batch_size=config["batch_size"],
#         shuffle=True,
#         num_workers=config.get("num_workers", 4), 
#         pin_memory=True
#     )
#     return dataloader

def get_dataloader(file_path, config, word2idx, word_freq, mode="ns", path_table=None, code_table=None):
    
    if mode == "ns":
        dataset = SkipGramNSIterableDataset(
            file_path=TOKEN_INDICES_PATH, # ğŸ‘ˆ íŒŒì¼ ê²½ë¡œ ì „ë‹¬
            word2idx=word2idx, 
            word_freq=word_freq, # ğŸ‘ˆ ë¹ˆë„ìˆ˜ ì „ë‹¬
            neg_sample_size=config.get("neg_sample_size", 5),
            window_size=config["window_size"]
        )
        
    elif mode == "hs":
        # HS IterableDataset êµ¬í˜„ ì‹œ ì—¬ê¸°ì— ì¶”ê°€
        raise NotImplementedError("HS IterableDataset must be implemented for streaming.")
        
    else:
        raise ValueError("mode must be 'ns' or 'hs'")

    dataloader = DataLoader(
        dataset,
        batch_size=config["batch_size"],
        # IterableDatasetì€ ë°ì´í„° ìˆœì„œë¥¼ Dataset ë‚´ë¶€ì—ì„œ ì²˜ë¦¬í•˜ë¯€ë¡œ shuffle=False
        shuffle=False, 
        # OOM ë°©ì§€ë¥¼ ìœ„í•´ num_workersëŠ” 0ìœ¼ë¡œ ê¶Œì¥, íŠ¹íˆ íŒŒì¼ IOê°€ ë§ì„ ë•Œ
        num_workers=config.get("num_workers", 32), 
        pin_memory=True
    )
    return dataloader