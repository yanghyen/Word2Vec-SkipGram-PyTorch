#!/usr/bin/env python3
"""
ë°°ì¹˜ í‰ê°€ í…Œì´ë¸” ìƒì„± ìŠ¤í¬ë¦½íŠ¸: runs/checkpoints_nsì™€ runs/checkpoints_hsì˜ ëª¨ë“  .pth íŒŒì¼ë“¤ì„ í‰ê°€í•˜ì—¬ 
íŒŒì¼ëª…ì„ ì»¬ëŸ¼ìœ¼ë¡œ í•˜ëŠ” í•˜ë‚˜ì˜ CSV í…Œì´ë¸”ë¡œ ì €ì¥í•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    python src/batch_eval_table.py [--output results/batch_evaluation_table.csv]
    python src/batch_eval_table.py --checkpoint_dir runs/checkpoints_ns [--output results/batch_evaluation_table.csv]
"""

import os
import glob
import torch
import yaml
import numpy as np
import pandas as pd
import pickle
import argparse
import re
from pathlib import Path
from scipy.stats import spearmanr
from tqdm import tqdm
import sys

from model import SkipGram

def parse_checkpoint_name(checkpoint_path):
    """
    ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ëª…ì—ì„œ ì„¤ì • ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    
    ì˜ˆì‹œ: ns_window-5_sub-False_seed-123.pth -> ns, window=5, subsample=False, seed=123
    """
    filename = Path(checkpoint_path).stem
    
    info = {
        'mode': 'ns',
        'window': 5,
        'subsample': True,
        'seed': 42
    }
    
    # ëª¨ë“œ ì¶”ì¶œ
    if filename.startswith('hs'):
        info['mode'] = 'hs'
    elif filename.startswith('ns'):
        info['mode'] = 'ns'
    
    # window í¬ê¸° ì¶”ì¶œ
    window_match = re.search(r'window-(\d+)', filename)
    if window_match:
        info['window'] = int(window_match.group(1))
    
    # subsample ì„¤ì • ì¶”ì¶œ
    if 'sub-False' in filename:
        info['subsample'] = False
    elif 'sub-True' in filename:
        info['subsample'] = True
    
    # seed ì¶”ì¶œ
    seed_match = re.search(r'seed-(\d+)', filename)
    if seed_match:
        info['seed'] = int(seed_match.group(1))
    
    return info

def find_matching_config(checkpoint_info, configs_dir="configs"):
    """
    ì²´í¬í¬ì¸íŠ¸ ì •ë³´ì— ë§ëŠ” config íŒŒì¼ì„ ì°¾ìŠµë‹ˆë‹¤.
    """
    mode = checkpoint_info['mode']
    window = checkpoint_info['window']
    subsample = 'on' if checkpoint_info['subsample'] else 'off'
    seed = checkpoint_info['seed']
    
    # ê°€ëŠ¥í•œ config íŒŒì¼ëª… íŒ¨í„´ë“¤
    patterns = [
        f"{mode}_window-{window}_subsample-{subsample}_seed-{seed}.yaml",
        f"{mode}_window-{window}_subsample-{subsample}_seed-42.yaml",  # fallback
    ]
    
    for pattern in patterns:
        config_path = os.path.join(configs_dir, pattern)
        if os.path.exists(config_path):
            return config_path
    
    return None

def load_wordsim353(csv_path):
    """WordSim-353 ë°ì´í„°ì…‹ ë¡œë“œ"""
    pairs = []
    with open(csv_path, 'r', encoding='utf-8') as f:
        for line in f:
            parts = line.strip().split(',')
            if len(parts) >= 3:
                try:
                    w1, w2, score = parts[0], parts[1], float(parts[2])
                    pairs.append((w1, w2, score))
                except:
                    continue
    return pairs

def load_simlex999(txt_path):
    """SimLex-999 ë°ì´í„°ì…‹ ë¡œë“œ"""
    pairs = []
    with open(txt_path, 'r', encoding='utf-8') as f:
        next(f)  # í—¤ë” ìŠ¤í‚µ
        for line in f:
            parts = line.strip().split('\t')
            if len(parts) >= 4:
                try:
                    w1, w2, score = parts[0], parts[1], float(parts[3])
                    pairs.append((w1, w2, score))
                except:
                    continue
    return pairs

def load_google_analogy(txt_path):
    """Google Analogy ë°ì´í„°ì…‹ ë¡œë“œ"""
    analogies = []
    with open(txt_path, 'r', encoding='utf-8') as f:
        for line in f:
            if line.startswith(':'):
                continue
            parts = line.strip().split()
            if len(parts) == 4:
                analogies.append(tuple(parts))
    return analogies

def cosine_similarity_gpu(embeddings_tensor, v1_idx, v2_idx):
    """GPUë¥¼ ì‚¬ìš©í•œ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
    v1 = embeddings_tensor[v1_idx]
    v2 = embeddings_tensor[v2_idx]
    
    dot_product = torch.dot(v1, v2)
    norm_v1 = torch.norm(v1)
    norm_v2 = torch.norm(v2)
    
    if norm_v1 == 0 or norm_v2 == 0:
        return 0.0
    
    return (dot_product / (norm_v1 * norm_v2)).item()

def cosine_similarity(v1, v2):
    """CPUìš© ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° (ë°±ì—…ìš©)"""
    dot_product = np.dot(v1, v2)
    norm_v1 = np.linalg.norm(v1)
    norm_v2 = np.linalg.norm(v2)
    
    if norm_v1 == 0 or norm_v2 == 0:
        return 0
    
    return dot_product / (norm_v1 * norm_v2)

def analogy_gpu(embeddings_tensor, word2idx, a, b, c, topk=1, device='cuda'):
    """GPUë¥¼ ì‚¬ìš©í•œ ë‹¨ì–´ ìœ ì¶” ìˆ˜í–‰: a is to b as c is to ?"""
    if a not in word2idx or b not in word2idx or c not in word2idx:
        return []
    
    # ë²¡í„° ì—°ì‚°: king - man + woman = queen
    vec_result = embeddings_tensor[word2idx[b]] - embeddings_tensor[word2idx[a]] + embeddings_tensor[word2idx[c]]
    
    # ì…ë ¥ ë‹¨ì–´ë“¤ì˜ ì¸ë±ìŠ¤
    exclude_indices = {word2idx[a], word2idx[b], word2idx[c]}
    
    # ëª¨ë“  ì„ë² ë”©ê³¼ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ë°°ì¹˜ë¡œ ê³„ì‚°
    # ì •ê·œí™”
    vec_result_norm = vec_result / torch.norm(vec_result)
    embeddings_norm = embeddings_tensor / torch.norm(embeddings_tensor, dim=1, keepdim=True)
    
    # ë°°ì¹˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
    similarities = torch.matmul(embeddings_norm, vec_result_norm)
    
    # ì…ë ¥ ë‹¨ì–´ë“¤ ì œì™¸
    for idx in exclude_indices:
        similarities[idx] = -float('inf')
    
    # ìƒìœ„ topk ê°œ ì°¾ê¸°
    _, top_indices = torch.topk(similarities, topk)
    
    # ê²°ê³¼ ë³€í™˜
    idx2word = {idx: word for word, idx in word2idx.items()}
    results = []
    for i in range(topk):
        idx = top_indices[i].item()
        sim = similarities[idx].item()
        if idx in idx2word:
            results.append((idx2word[idx], sim))
    
    return results

def analogy(embeddings, word2idx, a, b, c, topk=1):
    """CPUìš© ë‹¨ì–´ ìœ ì¶” ìˆ˜í–‰ (ë°±ì—…ìš©)"""
    if a not in word2idx or b not in word2idx or c not in word2idx:
        return []
    
    # ë²¡í„° ì—°ì‚°: king - man + woman = queen
    vec_result = embeddings[word2idx[b]] - embeddings[word2idx[a]] + embeddings[word2idx[c]]
    
    # ëª¨ë“  ë‹¨ì–´ì™€ì˜ ìœ ì‚¬ë„ ê³„ì‚°
    similarities = []
    for word, idx in word2idx.items():
        if word in [a, b, c]:  # ì…ë ¥ ë‹¨ì–´ë“¤ ì œì™¸
            continue
        sim = cosine_similarity(vec_result, embeddings[idx])
        similarities.append((word, sim))
    
    # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬
    similarities.sort(key=lambda x: x[1], reverse=True)
    return similarities[:topk]

def evaluate_similarity_gpu(embeddings_tensor, word2idx, pairs, device='cuda'):
    """GPUë¥¼ ì‚¬ìš©í•œ ìœ ì‚¬ë„ í‰ê°€"""
    preds, golds = [], []
    
    for w1, w2, gold in pairs:
        if w1 in word2idx and w2 in word2idx:
            sim = cosine_similarity_gpu(embeddings_tensor, word2idx[w1], word2idx[w2])
            preds.append(sim)
            golds.append(gold)
    
    if len(preds) == 0:
        return 0.0
    
    corr, _ = spearmanr(preds, golds)
    return corr if not np.isnan(corr) else 0.0

def evaluate_similarity(embeddings, word2idx, pairs):
    """CPUìš© ìœ ì‚¬ë„ í‰ê°€ (ë°±ì—…ìš©)"""
    preds, golds = [], []
    
    for w1, w2, gold in pairs:
        if w1 in word2idx and w2 in word2idx:
            sim = cosine_similarity(embeddings[word2idx[w1]], embeddings[word2idx[w2]])
            preds.append(sim)
            golds.append(gold)
    
    if len(preds) == 0:
        return 0.0
    
    corr, _ = spearmanr(preds, golds)
    return corr if not np.isnan(corr) else 0.0

def evaluate_analogy_gpu(embeddings_tensor, word2idx, analogies, device='cuda'):
    """GPUë¥¼ ì‚¬ìš©í•œ ìœ ì¶” í‰ê°€"""
    total, correct = 0, 0
    for a, b, c, d_true in tqdm(analogies, desc="ğŸ”¸ GPU Analogy", ncols=100):
        preds = analogy_gpu(embeddings_tensor, word2idx, a, b, c, topk=1, device=device)
        if not preds:
            continue
        total += 1
        if preds[0][0] == d_true:
            correct += 1
    
    return correct / total if total > 0 else 0.0

def evaluate_analogy(embeddings, word2idx, analogies):
    """CPUìš© ìœ ì¶” í‰ê°€ (ë°±ì—…ìš©)"""
    total, correct = 0, 0
    
    for a, b, c, d_true in analogies:
        preds = analogy(embeddings, word2idx, a, b, c, topk=1)
        if not preds:
            continue
        total += 1
        if preds[0][0] == d_true:
            correct += 1
    
    return correct / total if total > 0 else 0.0

def evaluate_single_model(checkpoint_path, config_path):
    """ë‹¨ì¼ ëª¨ë¸ í‰ê°€"""
    print(f"ğŸ“„ í‰ê°€ ì¤‘: {Path(checkpoint_path).name}")
    print(f"   Config: {config_path}")
    
    # Config ë¡œë“œ
    try:
        with open(config_path, "r") as f:
            config = yaml.safe_load(f)
        print(f"   âœ… Config ë¡œë“œ ì™„ë£Œ")
    except Exception as e:
        print(f"   âŒ Config ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # Vocab ë¡œë“œ
    if config["training_mode"] == "hs":
        vocab_path = "runs/vocab_hs.pkl"
    else:
        vocab_path = "data/pretrain/vocab_data_3.pkl"
    
    print(f"   ğŸ“š Vocab ë¡œë”©: {vocab_path}")
    try:
        with open(vocab_path, "rb") as f:
            vocab_data = pickle.load(f)
        print(f"   âœ… Vocab ë¡œë“œ ì™„ë£Œ")
    except Exception as e:
        print(f"   âŒ Vocab ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise
    
    vocab, word2idx, idx2word = vocab_data["vocab"], vocab_data["word2idx"], vocab_data["idx2word"]
    
    # ëª¨ë¸ ë¡œë“œ
    vocab_size = len(vocab)
    embedding_dim = config["embedding_dim"]
    mode = config.get("training_mode", "ns").lower()
    
    print(f"   ğŸ§  ëª¨ë¸ ìƒì„±: vocab_size={vocab_size}, embedding_dim={embedding_dim}, mode={mode}")
    try:
        model = SkipGram(vocab_size, embedding_dim, mode=mode).to(device)
        print(f"   âœ… ëª¨ë¸ ìƒì„± ì™„ë£Œ")
        
        print(f"   ğŸ“¦ ì²´í¬í¬ì¸íŠ¸ ë¡œë”©: {checkpoint_path}")
        checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=True)
        model.load_state_dict(checkpoint["model_state_dict"])
        model.eval()
        print(f"   âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    except Exception as e:
        print(f"   âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise
    
    # ì„ë² ë”© ì¶”ì¶œ (GPU ë²„ì „ê³¼ CPU ë²„ì „ ëª¨ë‘ ì¤€ë¹„)
    print(f"   ğŸ”¢ ì„ë² ë”© ì¶”ì¶œ ì¤‘...")
    try:
        with torch.no_grad():
            embeddings_tensor = model.in_embeddings.weight.detach()  # GPUì— ìœ ì§€
            embeddings = embeddings_tensor.cpu().numpy()  # CPU ë°±ì—…ìš©
        print(f"   âœ… ì„ë² ë”© ì¶”ì¶œ ì™„ë£Œ: {embeddings.shape}")
        print(f"   ğŸš€ GPU ê°€ì† ì‚¬ìš©: {device}")
    except Exception as e:
        print(f"   âŒ ì„ë² ë”© ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        raise
    
    # ë°ì´í„°ì…‹ ë¡œë“œ
    print(f"   ğŸ“Š ë°ì´í„°ì…‹ ë¡œë”© ì¤‘...")
    try:
        wordsim_pairs = load_wordsim353("data/word_similarity/combined.csv")
        print(f"   âœ… WordSim-353 ë¡œë“œ: {len(wordsim_pairs)}ê°œ ìŒ")
        
        simlex_pairs = load_simlex999("data/word_similarity/SimLex-999/SimLex-999.txt")
        print(f"   âœ… SimLex-999 ë¡œë“œ: {len(simlex_pairs)}ê°œ ìŒ")
        
        analogy_pairs = load_google_analogy("data/word_similarity/word2vec/trunk/questions-words.txt")
        print(f"   âœ… Google Analogy ë¡œë“œ: {len(analogy_pairs)}ê°œ ìŒ")
    except Exception as e:
        print(f"   âŒ ë°ì´í„°ì…‹ ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise
    
    # í‰ê°€ ìˆ˜í–‰ (GPU ê°€ì† ì‚¬ìš©)
    print(f"   ğŸ¯ í‰ê°€ ì‹œì‘...")
    results = {}
    
    try:
        print(f"   ğŸ“Š WordSim-353 í‰ê°€ ì¤‘... (GPU ê°€ì†)")
        results["WordSim-353"] = evaluate_similarity_gpu(embeddings_tensor, word2idx, wordsim_pairs, device)
        print(f"   âœ… WordSim-353: {results['WordSim-353']:.4f}")
        
        print(f"   ğŸ“˜ SimLex-999 í‰ê°€ ì¤‘... (GPU ê°€ì†)")
        results["SimLex-999"] = evaluate_similarity_gpu(embeddings_tensor, word2idx, simlex_pairs, device)
        print(f"   âœ… SimLex-999: {results['SimLex-999']:.4f}")
        
        print(f"   ğŸ‘‘ Google Analogy í‰ê°€ ì¤‘... (GPU ê°€ì†ìœ¼ë¡œ ë¹¨ë¼ì§‘ë‹ˆë‹¤!)")
        results["Google Analogy"] = evaluate_analogy_gpu(embeddings_tensor, word2idx, analogy_pairs, device)
        print(f"   âœ… Google Analogy: {results['Google Analogy']:.4f}")
        
    except Exception as e:
        print(f"   âš ï¸ GPU í‰ê°€ ì‹¤íŒ¨, CPUë¡œ ëŒ€ì²´: {e}")
        # GPU ì‹¤íŒ¨ì‹œ CPU ë°±ì—…
        print(f"   ğŸ“Š WordSim-353 í‰ê°€ ì¤‘... (CPU)")
        results["WordSim-353"] = evaluate_similarity(embeddings, word2idx, wordsim_pairs)
        print(f"   âœ… WordSim-353: {results['WordSim-353']:.4f}")
        
        print(f"   ğŸ“˜ SimLex-999 í‰ê°€ ì¤‘... (CPU)")
        results["SimLex-999"] = evaluate_similarity(embeddings, word2idx, simlex_pairs)
        print(f"   âœ… SimLex-999: {results['SimLex-999']:.4f}")
        
        print(f"   ğŸ‘‘ Google Analogy í‰ê°€ ì¤‘... (CPU - ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦½ë‹ˆë‹¤)")
        results["Google Analogy"] = evaluate_analogy(embeddings, word2idx, analogy_pairs)
        print(f"   âœ… Google Analogy: {results['Google Analogy']:.4f}")
    
    print(f"   ğŸ‰ í‰ê°€ ì™„ë£Œ!")
    return results

def main():
    parser = argparse.ArgumentParser(description="ë°°ì¹˜ í‰ê°€ í…Œì´ë¸” ìƒì„±")
    parser.add_argument("--checkpoint_dir", default=None, help="ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ë“¤ì´ ìˆëŠ” ë””ë ‰í† ë¦¬ (ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ runs/checkpoints_nsì™€ runs/checkpoints_hsì—ì„œ ìë™ ê²€ìƒ‰)")
    parser.add_argument("--configs_dir", default="configs", help="config íŒŒì¼ë“¤ì´ ìˆëŠ” ë””ë ‰í† ë¦¬")
    parser.add_argument("--output", default="results/batch_evaluation_table.csv", help="ì¶œë ¥ CSV íŒŒì¼ ê²½ë¡œ")
    
    args = parser.parse_args()
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(os.path.dirname(args.output), exist_ok=True)
    
    # .pth íŒŒì¼ë“¤ ì°¾ê¸°
    if args.checkpoint_dir:
        # íŠ¹ì • ë””ë ‰í† ë¦¬ì—ì„œ ì°¾ê¸°
        checkpoint_files = glob.glob(os.path.join(args.checkpoint_dir, "*.pth"))
        checkpoint_files.sort()
        print(f"ğŸ“ ì§€ì •ëœ ë””ë ‰í† ë¦¬ì—ì„œ {len(checkpoint_files)}ê°œì˜ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤: {args.checkpoint_dir}")
    else:
        # runs/checkpoints_nsì™€ runs/checkpoints_hsì—ì„œ ìë™ìœ¼ë¡œ ì°¾ê¸°
        checkpoint_files = []
        checkpoint_dirs = ["runs/checkpoints_ns", "runs/checkpoints_hs"]
        for ckpt_dir in checkpoint_dirs:
            if os.path.exists(ckpt_dir):
                files = glob.glob(os.path.join(ckpt_dir, "*.pth"))
                checkpoint_files.extend(files)
                if files:
                    print(f"ğŸ“ {ckpt_dir}ì—ì„œ {len(files)}ê°œì˜ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
        checkpoint_files.sort()
        print(f"ğŸ“ ì´ {len(checkpoint_files)}ê°œì˜ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
    
    # ê° ëª¨ë¸ í‰ê°€
    all_results = {}
    
    for checkpoint_path in tqdm(checkpoint_files, desc="ëª¨ë¸ í‰ê°€"):
        try:
            # ì²´í¬í¬ì¸íŠ¸ ì •ë³´ ì¶”ì¶œ
            checkpoint_info = parse_checkpoint_name(checkpoint_path)
            
            # ë§¤ì¹­ë˜ëŠ” config íŒŒì¼ ì°¾ê¸°
            config_path = find_matching_config(checkpoint_info, args.configs_dir)
            
            if not config_path:
                print(f"âŒ Config íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {checkpoint_path}")
                continue
            
            # ëª¨ë¸ í‰ê°€
            results = evaluate_single_model(checkpoint_path, config_path)
            
            # íŒŒì¼ëª…ì„ í‚¤ë¡œ ì‚¬ìš© (í™•ì¥ì ì œê±°)
            model_name = Path(checkpoint_path).stem
            all_results[model_name] = results
            
        except Exception as e:
            print(f"âŒ í‰ê°€ ì‹¤íŒ¨: {checkpoint_path}")
            print(f"   ì—ëŸ¬: {e}")
            continue
    
    # ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
    if not all_results:
        print("âŒ í‰ê°€ëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ë°ì´í„°ì…‹ì„ í–‰ìœ¼ë¡œ, ëª¨ë¸ì„ ì—´ë¡œ í•˜ëŠ” í…Œì´ë¸” ìƒì„±
    datasets = ["WordSim-353", "SimLex-999", "Google Analogy"]
    
    table_data = {}
    for dataset in datasets:
        table_data[dataset] = {}
        for model_name, results in all_results.items():
            table_data[dataset][model_name] = results.get(dataset, 0.0)
    
    # DataFrame ìƒì„±
    df = pd.DataFrame(table_data).T  # ì „ì¹˜í•˜ì—¬ ë°ì´í„°ì…‹ì´ í–‰ì´ ë˜ë„ë¡
    
    # CSV ì €ì¥
    df.to_csv(args.output)
    
    print(f"\nâœ… ë°°ì¹˜ í‰ê°€ ì™„ë£Œ!")
    print(f"ğŸ“Š í‰ê°€ëœ ëª¨ë¸ ìˆ˜: {len(all_results)}")
    print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥ë¨: {args.output}")
    
    # ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°
    print(f"\nğŸ“‹ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°:")
    print(df.round(4))

if __name__ == "__main__":
    main()
