#!/usr/bin/env python3
"""
ë°°ì¹˜ í‰ê°€ ê²°ê³¼ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸: ì—¬ëŸ¬ ëª¨ë¸ì˜ í‰ê°€ ê²°ê³¼ë¥¼ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    python scripts/analyze_results.py --results_dir results/batch_eval
"""

import os
import glob
import pandas as pd
import argparse
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import re

def parse_model_info_from_filename(filename):
    """
    íŒŒì¼ëª…ì—ì„œ ëª¨ë¸ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    
    ì˜ˆì‹œ: ns_window-5_sub-False__step-2000000_results.csv
    """
    stem = Path(filename).stem.replace('_results', '')
    
    info = {
        'mode': 'ns',
        'window': 5,
        'subsample': True,
        'seed': 42,
        'epoch': None,
        'step': None,
        'model_name': stem
    }
    
    # ëª¨ë“œ ì¶”ì¶œ
    if stem.startswith('hs'):
        info['mode'] = 'hs'
    elif stem.startswith('ns'):
        info['mode'] = 'ns'
    
    # window í¬ê¸° ì¶”ì¶œ
    window_match = re.search(r'window-(\d+)', stem)
    if window_match:
        info['window'] = int(window_match.group(1))
    
    # subsample ì„¤ì • ì¶”ì¶œ
    if 'sub-False' in stem:
        info['subsample'] = False
    elif 'sub-True' in stem:
        info['subsample'] = True
    
    # seed ì¶”ì¶œ
    seed_match = re.search(r'seed-(\d+)', stem)
    if seed_match:
        info['seed'] = int(seed_match.group(1))
    
    # epoch ë˜ëŠ” step ì¶”ì¶œ
    epoch_match = re.search(r'epoch-(\d+)', stem)
    if epoch_match:
        info['epoch'] = int(epoch_match.group(1))
    
    step_match = re.search(r'step-(\d+)', stem)
    if step_match:
        info['step'] = int(step_match.group(1))
    
    return info

def load_all_results(results_dir):
    """
    ê²°ê³¼ ë””ë ‰í† ë¦¬ì—ì„œ ëª¨ë“  CSV íŒŒì¼ì„ ë¡œë“œí•˜ê³  í†µí•©í•©ë‹ˆë‹¤.
    """
    csv_files = glob.glob(os.path.join(results_dir, "*_results.csv"))
    
    all_results = []
    
    for csv_file in csv_files:
        try:
            df = pd.read_csv(csv_file)
            model_info = parse_model_info_from_filename(csv_file)
            
            # ê° í–‰ì— ëª¨ë¸ ì •ë³´ ì¶”ê°€
            for _, row in df.iterrows():
                result_row = {
                    'model_name': model_info['model_name'],
                    'mode': model_info['mode'],
                    'window': model_info['window'],
                    'subsample': model_info['subsample'],
                    'seed': model_info['seed'],
                    'epoch': model_info['epoch'],
                    'step': model_info['step'],
                    'dataset': row['Dataset'],
                    'metric': row['Metric'],
                    'score': float(row['Score'])
                }
                all_results.append(result_row)
                
        except Exception as e:
            print(f"âŒ {csv_file} ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    return pd.DataFrame(all_results)

def create_comparison_plots(df, output_dir):
    """
    ë¹„êµ ë¶„ì„ì„ ìœ„í•œ ì‹œê°í™”ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    plt.style.use('default')
    fig_size = (15, 10)
    
    # 1. ë°ì´í„°ì…‹ë³„ ì„±ëŠ¥ ë¹„êµ (ëª¨ë“œë³„)
    plt.figure(figsize=fig_size)
    
    datasets = df['dataset'].unique()
    modes = df['mode'].unique()
    
    for i, dataset in enumerate(datasets, 1):
        plt.subplot(2, 2, i)
        
        dataset_df = df[df['dataset'] == dataset]
        
        # ëª¨ë“œë³„ ë°•ìŠ¤í”Œë¡¯
        sns.boxplot(data=dataset_df, x='mode', y='score', hue='window')
        plt.title(f'{dataset} Performance by Mode and Window Size')
        plt.ylabel('Score')
        
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'performance_by_mode_window.png'), dpi=300, bbox_inches='tight')
    plt.close()
    
    # 2. Subsample íš¨ê³¼ ë¶„ì„
    plt.figure(figsize=fig_size)
    
    for i, dataset in enumerate(datasets, 1):
        plt.subplot(2, 2, i)
        
        dataset_df = df[df['dataset'] == dataset]
        
        sns.boxplot(data=dataset_df, x='subsample', y='score', hue='mode')
        plt.title(f'{dataset}: Subsampling Effect')
        plt.ylabel('Score')
        
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'subsampling_effect.png'), dpi=300, bbox_inches='tight')
    plt.close()
    
    # 3. ì „ì²´ ì„±ëŠ¥ íˆíŠ¸ë§µ
    plt.figure(figsize=(12, 8))
    
    # í”¼ë²— í…Œì´ë¸” ìƒì„± (í‰ê·  ì ìˆ˜)
    pivot_df = df.groupby(['mode', 'window', 'subsample', 'dataset'])['score'].mean().reset_index()
    
    # ê° ë°ì´í„°ì…‹ë³„ë¡œ íˆíŠ¸ë§µ ìƒì„±
    for i, dataset in enumerate(datasets, 1):
        plt.subplot(2, 2, i)
        
        dataset_pivot = pivot_df[pivot_df['dataset'] == dataset]
        heatmap_data = dataset_pivot.pivot_table(
            values='score', 
            index=['mode', 'subsample'], 
            columns='window'
        )
        
        sns.heatmap(heatmap_data, annot=True, fmt='.3f', cmap='viridis')
        plt.title(f'{dataset} Average Scores')
        
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'performance_heatmap.png'), dpi=300, bbox_inches='tight')
    plt.close()

def generate_summary_report(df, output_dir):
    """
    ì¢…í•© ë¶„ì„ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    report_path = os.path.join(output_dir, 'analysis_report.md')
    
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("# Word2Vec ëª¨ë¸ í‰ê°€ ê²°ê³¼ ë¶„ì„ ë¦¬í¬íŠ¸\n\n")
        
        # ê¸°ë³¸ í†µê³„
        f.write("## ğŸ“Š ê¸°ë³¸ í†µê³„\n\n")
        f.write(f"- ì´ í‰ê°€ëœ ëª¨ë¸ ìˆ˜: {df['model_name'].nunique()}\n")
        f.write(f"- í‰ê°€ ë°ì´í„°ì…‹: {', '.join(df['dataset'].unique())}\n")
        f.write(f"- í›ˆë ¨ ëª¨ë“œ: {', '.join(df['mode'].unique())}\n")
        f.write(f"- ìœˆë„ìš° í¬ê¸°: {', '.join(map(str, sorted(df['window'].unique())))}\n\n")
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ë“¤
        f.write("## ğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸ë“¤\n\n")
        
        for dataset in df['dataset'].unique():
            dataset_df = df[df['dataset'] == dataset]
            best_model = dataset_df.loc[dataset_df['score'].idxmax()]
            
            f.write(f"### {dataset}\n")
            f.write(f"- **ëª¨ë¸**: {best_model['model_name']}\n")
            f.write(f"- **ì ìˆ˜**: {best_model['score']:.4f}\n")
            f.write(f"- **ì„¤ì •**: {best_model['mode'].upper()}, window={best_model['window']}, subsample={best_model['subsample']}\n\n")
        
        # ëª¨ë“œë³„ í‰ê·  ì„±ëŠ¥
        f.write("## ğŸ“ˆ ëª¨ë“œë³„ í‰ê·  ì„±ëŠ¥\n\n")
        
        mode_performance = df.groupby(['mode', 'dataset'])['score'].mean().reset_index()
        
        for dataset in df['dataset'].unique():
            f.write(f"### {dataset}\n")
            dataset_perf = mode_performance[mode_performance['dataset'] == dataset]
            
            for _, row in dataset_perf.iterrows():
                f.write(f"- **{row['mode'].upper()}**: {row['score']:.4f}\n")
            f.write("\n")
        
        # ìœˆë„ìš° í¬ê¸°ë³„ ì„±ëŠ¥
        f.write("## ğŸ” ìœˆë„ìš° í¬ê¸°ë³„ ì„±ëŠ¥\n\n")
        
        window_performance = df.groupby(['window', 'dataset'])['score'].mean().reset_index()
        
        for dataset in df['dataset'].unique():
            f.write(f"### {dataset}\n")
            dataset_perf = window_performance[window_performance['dataset'] == dataset]
            
            for _, row in dataset_perf.iterrows():
                f.write(f"- **Window {row['window']}**: {row['score']:.4f}\n")
            f.write("\n")
        
        # Subsampling íš¨ê³¼
        f.write("## âš¡ Subsampling íš¨ê³¼\n\n")
        
        subsample_performance = df.groupby(['subsample', 'dataset'])['score'].mean().reset_index()
        
        for dataset in df['dataset'].unique():
            f.write(f"### {dataset}\n")
            dataset_perf = subsample_performance[subsample_performance['dataset'] == dataset]
            
            for _, row in dataset_perf.iterrows():
                subsample_str = "ON" if row['subsample'] else "OFF"
                f.write(f"- **Subsample {subsample_str}**: {row['score']:.4f}\n")
            f.write("\n")
    
    print(f"ğŸ“‹ ë¶„ì„ ë¦¬í¬íŠ¸ ì €ì¥ë¨: {report_path}")

def main():
    parser = argparse.ArgumentParser(description="ë°°ì¹˜ í‰ê°€ ê²°ê³¼ ë¶„ì„")
    parser.add_argument("--results_dir", default="results/batch_eval", help="ê²°ê³¼ CSV íŒŒì¼ë“¤ì´ ìˆëŠ” ë””ë ‰í† ë¦¬")
    parser.add_argument("--output_dir", default="results/analysis", help="ë¶„ì„ ê²°ê³¼ë¥¼ ì €ì¥í•  ë””ë ‰í† ë¦¬")
    
    args = parser.parse_args()
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(args.output_dir, exist_ok=True)
    
    print(f"ğŸ“ ê²°ê³¼ ë¡œë”© ì¤‘: {args.results_dir}")
    
    # ëª¨ë“  ê²°ê³¼ ë¡œë“œ
    df = load_all_results(args.results_dir)
    
    if df.empty:
        print("âŒ ë¡œë“œëœ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ê²°ê³¼ ë””ë ‰í† ë¦¬ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return
    
    print(f"âœ… {len(df)}ê°œì˜ ê²°ê³¼ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
    print(f"ğŸ“Š {df['model_name'].nunique()}ê°œì˜ ê³ ìœ  ëª¨ë¸")
    
    # í†µí•© ê²°ê³¼ ì €ì¥
    combined_path = os.path.join(args.output_dir, 'combined_results.csv')
    df.to_csv(combined_path, index=False)
    print(f"ğŸ’¾ í†µí•© ê²°ê³¼ ì €ì¥ë¨: {combined_path}")
    
    # ì‹œê°í™” ìƒì„±
    print("ğŸ“ˆ ì‹œê°í™” ìƒì„± ì¤‘...")
    create_comparison_plots(df, args.output_dir)
    
    # ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±
    print("ğŸ“‹ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
    generate_summary_report(df, args.output_dir)
    
    print(f"\nâœ… ë¶„ì„ ì™„ë£Œ! ê²°ê³¼ëŠ” {args.output_dir}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()

